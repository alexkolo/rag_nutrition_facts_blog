[embeddings]
# Device (like "cuda", "cpu", "mps", "npu") that should be used for computation
device = "cpu"

# Pretrained Models : https://www.sbert.net/docs/sentence_transformer/pretrained_models.html
#
# `multi-qa-MiniLM-L6-cos-v1`
model_name = "multi-qa-MiniLM-L6-cos-v1"
# - https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1
# - It was "tuned for semantic search: Given a query/question, it can find relevant passages. It was trained on a large and diverse set of (question, answer) pairs."
n_dim_vec = 384 # Dimensions of the output vectors

# Token limits of the model
# "there is a limit of 512 word pieces: Text longer than that will be truncated. Further note that the model was just trained on input text up to 250 word pieces. It might not work well for longer text. "
n_token_max = 250 # recommended 250
# 1 token per 4-6 characters for english

# pharagraph chunking setup (calibrated to the embedding model)
n_char_max = 1000 # The maximum number of characters of each chunk. Default is 1000 (~250 tokens).
overlap = 100 # The number of characters to overlap between chunks. Default is 100 (~25 tokens).


[knowledge_base]
table_name = "table_simple03"
