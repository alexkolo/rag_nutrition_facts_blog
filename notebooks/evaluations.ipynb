{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations\n",
    "\n",
    "## TODO\n",
    "\n",
    "- change default table to `table_simple05`\n",
    "- use hybrid search with reranker `RRFReranker`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/repos/rag_nutrition_facts_blog/tvenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from collections.abc import Callable\n",
    "from datetime import datetime\n",
    "\n",
    "import lancedb\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from lancedb.rerankers import (\n",
    "    CohereReranker,\n",
    "    ColbertReranker,\n",
    "    CrossEncoderReranker,\n",
    "    LinearCombinationReranker,\n",
    "    RRFReranker,\n",
    ")\n",
    "from matplotlib import pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from src.constants import LANCEDB_URI, POST_JSON_PATH, REPO_PATH, get_rag_config\n",
    "from src.ingestion import create_title_hash\n",
    "\n",
    "# to ignore FutureWarning from `transformers/tokenization_utils_base.py`\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/alex/repos/rag_nutrition_facts_blog')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REPO_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"table_simple05\"\n",
    "\n",
    "# secrets\n",
    "load_dotenv(REPO_PATH)\n",
    "cohere_api_key: str = os.getenv(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = lancedb.connect(uri=LANCEDB_URI)\n",
    "print(f\"List of all tables in the LanceDB database: {db.table_names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the table 'table_simple05': 7023\n"
     ]
    }
   ],
   "source": [
    "table = db.open_table(table_name)\n",
    "print(f\"Number of entries in the table '{table_name}': {table.count_rows()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full-text search (aka keyword-based search)\n",
    "# - https://lancedb.github.io/lancedb/fts/\n",
    "# table.create_fts_index([\"text\"], replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the index - you need to have enough data in the table for an effective training step\n",
    "# (takes ~one minute)\n",
    "# dot faster than cosine\n",
    "device: str = get_rag_config()[\"embeddings\"][\"device\"]\n",
    "emb_model_metric: str = get_rag_config()[\"embeddings\"][\"metric\"]\n",
    "# table.create_index(metric=emb_model_metric, replace=True, accelerator=\"cuda\" if device == \"cuda\" else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Blog entry titles as queries\n",
    "\n",
    "- try to find titles that are very similar to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Guidelines:\n",
    "\n",
    "- **Cosine Similarity = 1.0**: The titles are **exactly identical**.\n",
    "- **Cosine Similarity > 0.9**: The titles are **highly similar**, often differing only slightly, such as small word variations, synonyms, or rewording that conveys essentially the same idea. This is where titles likely \"feel identical\" to most readers.\n",
    "- **Cosine Similarity between 0.7 and 0.9**: The titles are **similar but not identical**. They likely discuss similar topics or concepts but may have different wording or slight shifts in focus. Readers may perceive these as very similar or interchangeable, but subtle differences might exist.\n",
    "- **Cosine Similarity < 0.7**: The titles are **noticeably different**. They may refer to related topics but convey distinct ideas or focus. Readers would likely view them as different, though related.\n",
    "\n",
    "Threshold for Perceived Identicalness: For most **human readers**, a cosine similarity score between **0.9 and 1.0** typically corresponds to titles that feel **identical or extremely similar**. This range indicates that the titles share most of their key terms and structure, with only minor variations.\n",
    "\n",
    "\n",
    "#### Factors Affecting Perceived Similarity:\n",
    "- **Synonyms and Phrasing**: Different words or phrases with similar meanings (e.g., \"guide\" vs. \"how to\").\n",
    "- **Order of Words**: Titles with the same words but in a different order can have a high similarity score and feel identical to readers.\n",
    "- **Stopwords**: Removing common words (e.g., \"the,\" \"a\") in pre-processing can affect the similarity score but might not affect the reader's perception.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cosine Similarity between Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all titles (1281 entries)\n",
    "file_list = list(POST_JSON_PATH.glob(\"*.json\"))\n",
    "\n",
    "ignore_titles: list[str] = [\n",
    "    \"Year-in-Review Presentation\",\n",
    "    \"Thank You for Your Support\",\n",
    "    \"DVD Now Available\",\n",
    "    \"Dr. Greger’s New DVD\",\n",
    "    \"Dr. Greger’s DVDs\",\n",
    "    \"Top 10 NutritionFacts.org Videos\",\n",
    "    \"Top 10 Most Popular Videos\",\n",
    "]\n",
    "ignore_list_lower: list[str] = [title.lower() for title in ignore_titles]\n",
    "\n",
    "titles: list[str] = []\n",
    "urls: list[str] = []\n",
    "created: list[str] = []\n",
    "pbar = tqdm(file_list[:])\n",
    "for json_file in pbar:\n",
    "    pbar.set_description(f\"{json_file.name[:40]:<40}\")\n",
    "\n",
    "    # load data\n",
    "    with open(json_file, encoding=\"utf-8\") as f:\n",
    "        blog_post: dict = json.load(f)\n",
    "    title: str = blog_post[\"title\"]\n",
    "\n",
    "    # ignore files without any paragraphs\n",
    "    if not blog_post[\"paragraphs\"]:\n",
    "        continue\n",
    "\n",
    "    # ignore tiles that contain string from `ignore_titles`\n",
    "    if any(ignored_str in title.lower() for ignored_str in ignore_list_lower):\n",
    "        continue\n",
    "\n",
    "    titles.append(blog_post[\"title\"])\n",
    "    urls.append(blog_post[\"url\"])\n",
    "    created.append(blog_post[\"created\"])\n",
    "# <3 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of titles: {len(titles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure similarity between titles\n",
    "emb_config: dict = get_rag_config()[\"embeddings\"]\n",
    "emb_model = SentenceTransformer(\n",
    "    emb_config[\"model_name\"],\n",
    "    device=emb_config[\"device\"],\n",
    "    similarity_fn_name=emb_config[\"metric\"],\n",
    ")\n",
    "titles_vec: list[list[float]] = emb_model.encode(titles, show_progress_bar=True)\n",
    "# <10 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_sim = pd.DataFrame(emb_model.similarity(titles_vec, titles_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plotting similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the similarity scores as pairs\n",
    "title_pairs = []\n",
    "similarity_scores = []\n",
    "for i in trange(titles_sim.shape[0]):\n",
    "    for j in range(i + 1, titles_sim.shape[1]):\n",
    "        title_pairs.append((titles_sim.index[i], titles_sim.columns[j]))\n",
    "        similarity_scores.append(titles_sim.iloc[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.scatter(range(len(similarity_scores)), similarity_scores, alpha=0.5, s=1)\n",
    "plt.title(\"Pairwise Similarity Scores Between Titles\")\n",
    "plt.xlabel(\"Title Pairs (Index)\")\n",
    "plt.ylabel(\"Similarity Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute average histogram and bin edges\n",
    "\n",
    "\n",
    "def compute_avg_hist(df: pd.DataFrame, bins=30, range_min=0, range_max=1, cumulative=False):\n",
    "    # Step 1: Create consistent bin edges for all titles\n",
    "    bin_edges = np.linspace(range_min, range_max, bins + 1)\n",
    "\n",
    "    # Step 2: Initialize list to hold the histograms\n",
    "    hist_data = []\n",
    "\n",
    "    # Set the diagonal values to NaN or exclude them\n",
    "    df_no_diag = df.copy()\n",
    "    np.fill_diagonal(df_no_diag.values, np.nan)\n",
    "\n",
    "    for i in range(df_no_diag.shape[0]):\n",
    "        # Exclude the NaN values (self-similarity) from the row\n",
    "        row_data = df_no_diag.iloc[i, :].dropna()\n",
    "\n",
    "        # Step 3: Compute the regular histogram for this row\n",
    "        hist, _ = np.histogram(row_data, bins=bin_edges, density=True)\n",
    "\n",
    "        if cumulative:\n",
    "            # Step 4: Compute the cumulative sum, starting from the highest bin to the lowest\n",
    "            hist = np.cumsum(hist[::-1])[::-1]  # Reverse to high-to-low cumulative sum\n",
    "\n",
    "        hist_data.append(hist)\n",
    "\n",
    "    # Step 5: Convert the list of histograms into a NumPy array for easier averaging\n",
    "    hist_data = np.array(hist_data)\n",
    "\n",
    "    # Step 6: Compute the average histogram across all rows\n",
    "    avg_hist = hist_data.mean(axis=0)\n",
    "\n",
    "    return avg_hist, bin_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average histogram and bin edges\n",
    "avg_hist, bin_edges = compute_avg_hist(titles_sim, bins=100, range_min=titles_sim.min().min(), range_max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the averaged histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "x_bar = (bin_edges[:-1] + bin_edges[1:]) / 2  # Calculate the bin centers\n",
    "plt.bar(x=x_bar, height=avg_hist, width=(bin_edges[1] - bin_edges[0]), edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Averaged Histogram of Similarity Scores for All Titles\")\n",
    "plt.xlabel(\"Similarity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average Cumulative  histogram and bin edges\n",
    "avg_hist, bin_edges = compute_avg_hist(\n",
    "    titles_sim, bins=100, range_min=titles_sim.min().min(), range_max=1, cumulative=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the averaged histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "x_bar = (bin_edges[:-1] + bin_edges[1:]) / 2  # Calculate the bin centers\n",
    "height = avg_hist  # / avg_hist[0]\n",
    "plt.bar(x=x_bar, height=height, width=(bin_edges[1] - bin_edges[0]), edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Averaged Cumulative Histogram of Similarity Scores (High to Low)\")\n",
    "plt.xlabel(\"Similarity\")\n",
    "plt.ylabel(\"Cumulative Frequency (Density)\")\n",
    "plt.yscale(\"log\")\n",
    "# horizaontl line at one\n",
    "plt.axhline(y=1, color=\"r\", linestyle=\"-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find groups of titles with high Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_titles_by_similarity(df: pd.DataFrame, threshold: float = 0.9) -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    Groups titles based on cosine similarity scores in the DataFrame `df`.\n",
    "    Titles are grouped together if their similarity score is greater than `threshold`.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing pairwise similarity scores, where rows and columns represent titles.\n",
    "    threshold (float): The similarity score threshold for grouping titles (default is 0.9).\n",
    "\n",
    "    Returns:\n",
    "    List of lists, where each sublist contains indices of grouped titles.\n",
    "    \"\"\"\n",
    "    # Create a graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes (each title is a node)\n",
    "    titles = df.index\n",
    "    G.add_nodes_from(titles)\n",
    "\n",
    "    # Add edges between nodes if the similarity score is above the threshold\n",
    "    for i in trange(df.shape[0]):\n",
    "        for j in range(i + 1, df.shape[1]):\n",
    "            if df.iloc[i, j] > threshold:\n",
    "                G.add_edge(titles[i], titles[j])\n",
    "\n",
    "    # Find connected components (groups of titles) and filter out groups with only one title\n",
    "    groups = [group for component in nx.connected_components(G) if len(group := list(component)) > 1]\n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups: list[list[int]] = group_titles_by_similarity(titles_sim, threshold=0.8)\n",
    "# <30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the grouped titles\n",
    "print(f\"Number of groups: {len(groups)}\")\n",
    "for g, group in enumerate(groups):\n",
    "    print(f\"{g}. Group:\", group)\n",
    "    dates = [datetime.fromisoformat(created[i]) for i in group]\n",
    "    # order by date\n",
    "    order = np.argsort(dates)\n",
    "    group = [group[i] for i in order]\n",
    "\n",
    "    i_ref: int = group[-1]  # take most recent title\n",
    "    for i_title in group:\n",
    "        print(f\"\\t({titles_sim.iloc[i_ref, i_title]:.2f}) ({created[i_title][:10]})  {titles[i_title]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save groups\n",
    "out = {}\n",
    "for g, group in enumerate(groups):\n",
    "    # print(f\"{g}. Group:\", group)\n",
    "    dates = [datetime.fromisoformat(created[i]) for i in group]\n",
    "    # order by date\n",
    "    order = np.argsort(dates)\n",
    "    group = [group[i] for i in order]\n",
    "    out[g] = [[create_title_hash({\"title\": titles[i_title], \"url\": urls[i_title]}) for i_title in group]]\n",
    "\n",
    "pd.DataFrame().from_dict(out, orient=\"index\").to_csv(\"eva_title_groups.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test hash filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with expected\n",
    "groups_from_file = pd.read_csv(\"eva_title_groups.csv\")\n",
    "for g, group in enumerate(groups):\n",
    "    expected = {titles[i_title] for i_title in group}\n",
    "    hash_filter: str = groups_from_file.iloc[g, 0].replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    resp = set(table.search().where(f\"hash_title in ({hash_filter})\").limit(100).to_pandas()[\"title\"].to_list())\n",
    "    if resp != expected:\n",
    "        print(f\"Expected: {expected}\")\n",
    "        print(f\"Response: {resp}\")\n",
    "        print(hash_filter)\n",
    "        group_urls = [urls[i_title] for i_title in group]\n",
    "        print(group_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create ground truth\n",
    "\n",
    "- csv: title, hash_title, allowed_hashes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_list(s: str) -> list[str]:\n",
    "    return s.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").split(\", \")\n",
    "\n",
    "\n",
    "title_groups: list[str] = pd.read_csv(\"eva_title_groups.csv\").iloc[:, 0].apply(str_to_list).to_list()\n",
    "\n",
    "ground_truth: list[dict] = []\n",
    "for t, title in enumerate(titles[:]):\n",
    "    hash_title = create_title_hash({\"title\": title, \"url\": urls[t]})\n",
    "    allowed_hashes = {hash_title}\n",
    "    # check if hash appears in any group from `title_groups`\n",
    "    for group in title_groups:\n",
    "        if hash_title in group:\n",
    "            # add group hashes to `allowed_hases`\n",
    "            allowed_hashes.update(group)\n",
    "\n",
    "    ground_truth.append({\"title\": title.strip(), \"hash_title\": hash_title, \"allowed_hashes\": list(allowed_hashes)})\n",
    "\n",
    "# save to file\n",
    "pd.DataFrame(ground_truth).to_csv(\"eva_ground_truth.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever Evaluation\n",
    "\n",
    "- titles are user queries\n",
    "- a query should return a text chunk from the same blog post or from another blog post with similar cosine similarity in respect to the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_relevance(\n",
    "    ground_truth: list[dict],\n",
    "    search_func: Callable[[], pd.DataFrame],\n",
    "    **kwargs,\n",
    ") -> list[list[bool]]:\n",
    "    relevance_total: list[list[bool]] = []\n",
    "    for entry in tqdm(ground_truth):\n",
    "        allowed_hashes: list[str] = entry[\"allowed_hashes\"]\n",
    "\n",
    "        title: str = entry[\"title\"]\n",
    "        query: str = title.lower().replace(\":\", \" \").replace(\"\\u00a0\", \" \").strip()\n",
    "        resp: pd.DataFrame = search_func(query=query, **kwargs)\n",
    "        retrieved_hashes: list[str] = resp[\"hash_title\"].to_list()\n",
    "        relevance: list[bool] = [h in allowed_hashes for h in retrieved_hashes]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return relevance_total\n",
    "\n",
    "\n",
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] is True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.read_csv(\"eva_ground_truth.csv\").to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic search methods: keyword (fts), vector, hybrid\n",
    "\n",
    "- hybrid uses \"Reciprocal Rank Fusion Reranker\"\n",
    "\n",
    "```python\n",
    "100%|██████████| 1248/1248 [00:10<00:00, 123.77it/s]\n",
    "fts - 0:00:10.086127 - 0.819 - 0.948\n",
    "100%|██████████| 1248/1248 [00:40<00:00, 30.87it/s] \n",
    "vector - 0:00:40.431775 - 0.809 - 1.055\n",
    "100%|██████████| 1248/1248 [00:55<00:00, 22.34it/s]\n",
    "hybrid - 0:00:55.859763 - 0.841 - 1.139\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1248/1248 [00:08<00:00, 142.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fts - 0:00:08.788253 - 0.819 - 0.948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1248/1248 [00:37<00:00, 33.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector - 0:00:37.410678 - 0.809 - 1.055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1248/1248 [00:45<00:00, 27.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hybrid - 0:00:45.072472 - 0.841 - 1.139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def basic_search(query: str, query_type: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    return table.search(query=query, query_type=query_type).limit(top_k).to_pandas()\n",
    "\n",
    "\n",
    "res_basic_search: dict[str, list[list[bool]]] = {}\n",
    "for query_type in [\"fts\", \"vector\", \"hybrid\"]:\n",
    "    t_start = datetime.now()\n",
    "    relevance: list[list[bool]] = measure_relevance(ground_truth, search_func=basic_search, query_type=query_type)\n",
    "    t_dur = datetime.now() - t_start\n",
    "    res_basic_search[query_type] = relevance\n",
    "    print(f\"{query_type} - {t_dur} - {hit_rate(relevance):.3f} - {mrr(relevance):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `LinearCombinationReranker`\n",
    "- https://lancedb.github.io/lancedb/reranking/linear_combination/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search_lc(query: str, top_k: int = 5, weight=0.7) -> pd.DataFrame:\n",
    "    reranker_lc = LinearCombinationReranker(weight=weight)\n",
    "    return table.search(query=query, query_type=\"hybrid\").rerank(reranker=reranker_lc).limit(top_k).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1248/1248 [00:47<00:00, 26.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8413461538461539, 1.138715277777783)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance_h07: list[list[bool]] = measure_relevance(ground_truth, search_func=hybrid_search_lc, weight=0.7)\n",
    "hit_rate(relevance_h07), mrr(relevance_h07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1248/1248 [01:03<00:00, 19.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8413461538461539, 1.138715277777783)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance_h03: list[list[bool]] = measure_relevance(ground_truth, search_func=hybrid_search_lc, weight=0.3)\n",
    "hit_rate(relevance_h03), mrr(relevance_h03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reciprocal Rank Fusion Reranker\n",
    "https://lancedb.github.io/lancedb/reranking/rrf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search_rrf(query: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    reranker = RRFReranker()\n",
    "    return table.search(query=query, query_type=\"hybrid\").rerank(reranker=reranker).limit(top_k).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1248/1248 [00:49<00:00, 24.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.84375, 1.1598023504273551)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance_rrf: list[list[bool]] = measure_relevance(ground_truth, search_func=hybrid_search_rrf)\n",
    "hit_rate(relevance_rrf), mrr(relevance_rrf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "```test\n",
    "               hit_rate       mrr\n",
    "rrf            0.843750  1.159802\n",
    "hybrid         0.841346  1.138715\n",
    "lc_weight_0.7  0.841346  1.138715\n",
    "lc_weight_0.3  0.841346  1.138715\n",
    "fts            0.818910  0.947983\n",
    "vector         0.809295  1.054688\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rrf</th>\n",
       "      <td>0.843750</td>\n",
       "      <td>1.159802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hybrid</th>\n",
       "      <td>0.841346</td>\n",
       "      <td>1.138715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc_weight_0.7</th>\n",
       "      <td>0.841346</td>\n",
       "      <td>1.138715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc_weight_0.3</th>\n",
       "      <td>0.841346</td>\n",
       "      <td>1.138715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fts</th>\n",
       "      <td>0.818910</td>\n",
       "      <td>0.947983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vector</th>\n",
       "      <td>0.809295</td>\n",
       "      <td>1.054688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               hit_rate       mrr\n",
       "rrf            0.843750  1.159802\n",
       "hybrid         0.841346  1.138715\n",
       "lc_weight_0.7  0.841346  1.138715\n",
       "lc_weight_0.3  0.841346  1.138715\n",
       "fts            0.818910  0.947983\n",
       "vector         0.809295  1.054688"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_all = {}\n",
    "res_all.update(res_basic_search)\n",
    "res_all[\"rrf\"] = relevance_rrf\n",
    "res_all[\"lc_weight_0.7\"] = relevance_h07\n",
    "res_all[\"lc_weight_0.3\"] = relevance_h03\n",
    "scores_all = {key: {\"hit_rate\": hit_rate(res_all[key]), \"mrr\": mrr(res_all[key])} for key in res_all}\n",
    "pd.DataFrame.from_dict(scores_all, orient=\"index\").sort_values(by=\"hit_rate\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reranker: Coher (rate limit too low)\n",
    "- https://lancedb.github.io/lancedb/reranking/cohere/\n",
    "- only trial api key:  10 API calls / minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_rr_cohere(query: str, query_type: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    reranker = CohereReranker(api_key=cohere_api_key)\n",
    "    return table.search(query=query, query_type=query_type).rerank(reranker=reranker).limit(top_k).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_cohere_res: dict[str, list[list[bool]]] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for q_type in [\"fts\"]:  # , \"vector\", \"hybrid\"]:\n",
    "#     rr_cohere_res[q_type] = measure_relevance(ground_truth, search_func=search_rr_cohere, query_type=q_type)\n",
    "#     print(q_type, hit_rate(rr_cohere_res[q_type]), mrr(rr_cohere_res[q_type]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reranker: Cross Encoder (very slow!)\n",
    "\n",
    "- https://lancedb.github.io/lancedb/reranking/cross_encoder/\n",
    "- Evaluation  very slow! >1h for fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_rr_cross(query: str, query_type: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    reranker = CrossEncoderReranker(device=\"cpu\")\n",
    "    return table.search(query=query, query_type=query_type).rerank(reranker=reranker).limit(top_k).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rr_cross_res: dict[str, list[list[bool]]] = {}\n",
    "# for q_type in [\"fts\", \"vector\", \"hybrid\"]:\n",
    "#     rr_cross_res[q_type] = measure_relevance(ground_truth, search_func=search_rr_cross, query_type=q_type)\n",
    "#     print(q_type, hit_rate(rr_cross_res[q_type]), mrr(rr_cross_res[q_type]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reranker: ColBERT  (slow)\n",
    "- https://lancedb.github.io/lancedb/reranking/colbert/\n",
    "- Evaluation  slow! >30min for fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_rr_colbert(query: str, query_type: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    reranker = ColbertReranker(device=\"cpu\")\n",
    "    return table.search(query=query, query_type=query_type).rerank(reranker=reranker).limit(top_k).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rr_colbert_res: dict[str, list[list[bool]]] = {}\n",
    "# for q_type in [\"fts\", \"vector\", \"hybrid\"]:\n",
    "#     rr_colbert_res[q_type] = measure_relevance(ground_truth, search_func=search_rr_cross, query_type=q_type)\n",
    "#     print(q_type, hit_rate(rr_colbert_res[q_type]), mrr(rr_colbert_res[q_type]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
