{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of chatbot answers\n",
    "\n",
    "## Evaluation Runs\n",
    "\n",
    "1. 09/09/2024: 77/77 [45:17<00:00, 35.29s/it] using only \"mistral\", reranker \"rrf\"\n",
    "2. `20240911_101327`: 77/77 [46:15<00:00, 36.05s/it] testing many models, reranker \"rrf\"\n",
    "   - show in the documentation\n",
    "3. `20240912_105808`: 77/77 [33:16<00:00, 25.93s/it] testing many models, reranker \"cross-encoder\"\n",
    "    - Not very different to previous run respite different reranker. Not surprising since it should be very easy to retrieve the correct text chunk for the selected titles\n",
    "4. `20240912_114908` : 77/77 [39:39<00:00, 30.90s/it] testing many models, reranker cross-encoder, with enriching top-retrieved blog post (enrich_first=True)\n",
    "   - Not very different to previous runs. Not surprising as the main text chunk should be sufficiently to give a good answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the external files every time before executing any cell\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from collections.abc import Iterable\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import lancedb\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "from lancedb.table import Table as KBaseTable\n",
    "from matplotlib import pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.constants import (\n",
    "    GROUND_TRUTH_FILE,\n",
    "    GROUND_TRUTH_PATH,\n",
    "    LANCEDB_URI,\n",
    "    REPO_PATH,\n",
    "    get_rag_config,\n",
    ")\n",
    "from src.llm_api import (\n",
    "    build_full_llm_chat_input,\n",
    "    get_llm_api_client_object,\n",
    "    get_model_list,\n",
    "    get_preferred_model,\n",
    ")\n",
    "from src.prompt_building import WELCOME_MSG, extract_context_from_msg\n",
    "\n",
    "# to ignore warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name: str = get_rag_config()[\"knowledge_base\"][\"table_name\"]\n",
    "emb_config: dict = get_rag_config()[\"embeddings\"]\n",
    "\n",
    "# LLM Parameters\n",
    "# -----------------------------\n",
    "llm_api_config: dict[str, Any] = get_rag_config()[\"llm\"]\n",
    "LLM_TEMP: float = llm_api_config[\"settings\"][\"model_temp\"]\n",
    "LLM_API_NAME: str = llm_api_config[\"settings\"][\"api_name\"]\n",
    "LLM_API_CONFIG: dict[str, Any] = llm_api_config[\"api\"][LLM_API_NAME]\n",
    "LLM_API_KEY_NAME: str = LLM_API_CONFIG[\"key_name\"]\n",
    "LLM_API_KEY_URL: str = LLM_API_CONFIG[\"key_url\"]\n",
    "\n",
    "\n",
    "# Secrets\n",
    "# -----------------------------\n",
    "load_dotenv(REPO_PATH)\n",
    "LLM_API_KEY: str = os.getenv(LLM_API_KEY_NAME)\n",
    "\n",
    "# Paths\n",
    "ts: str = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(f\"Evaluation Run Timestamp: {ts}\")\n",
    "eva_rag_results: Path = GROUND_TRUTH_PATH / f\"eva_rag_results_{ts}.json\"\n",
    "eva_rag_similarity: Path = GROUND_TRUTH_PATH / f\"eva_rag_similarity_{ts}.csv\"\n",
    "\n",
    "# retriever config\n",
    "retriever_config: dict = get_rag_config()[\"retriever\"]\n",
    "retriever_config[\"enrich_first\"] = True  # `False` for tests before 12/09/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db: lancedb.db.DBConnection = lancedb.connect(uri=LANCEDB_URI)\n",
    "print(f\"List of all tables in the LanceDB database: {db.table_names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_base: lancedb.table.Table = db.open_table(table_name)\n",
    "print(f\"Number of entries in the table '{table_name}': {k_base.count_rows()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure similarity between titles\n",
    "emb_model = SentenceTransformer(\n",
    "    emb_config[\"model_name\"],\n",
    "    device=emb_config[\"device\"],\n",
    "    similarity_fn_name=emb_config[\"metric\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take ground truth entries with where text chunk of the title has at least a cosine similarity of 0.8\n",
    "ground_truth = pd.read_csv(GROUND_TRUTH_FILE).loc[lambda df: df[\"best_doc_sim\"] > 0.8]\n",
    "print(f\"Number of entries in the ground truth: {len(ground_truth)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground Truth: Average Similarity of text chunk to its title\n",
    "print(f\"Average similarity: {ground_truth['best_doc_sim'].mean():.2f}+-{ground_truth['best_doc_sim'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_llm(api_key: str, api_name: str, api_config: dict, model_name: str = \"\") -> tuple[Groq, str]:\n",
    "    # Setup Model Name\n",
    "    if not model_name:\n",
    "        models_url: str = api_config.get(\"models\", {}).get(\"url\", \"\")\n",
    "        ranked_models: list[str] = api_config.get(\"models\", {}).get(\"ranked\", [])\n",
    "        model_name = get_preferred_model(api_key=api_key, models_url=models_url, ranked_models=ranked_models)\n",
    "\n",
    "    # Setup LLM API Client\n",
    "    llm_api_client = get_llm_api_client_object(api_name=api_name)\n",
    "    return llm_api_client(api_key=api_key), model_name\n",
    "\n",
    "\n",
    "# llm_api_client, model_name = connect_to_llm(api_key=LLM_API_KEY, api_name=LLM_API_NAME, api_config=LLM_API_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of models\n",
    "models_url: str = LLM_API_CONFIG.get(\"models\", {}).get(\"url\", \"\")\n",
    "models_full_list: list[dict] = get_model_list(api_key=LLM_API_KEY, models_url=models_url)\n",
    "# get active models\n",
    "models_excluded: list[str] = [\"whisper\", \"tool\", \"llava\", \"gemma-\", \"guard\", \"llama-3.1\"]\n",
    "# guard : classifier for safe or not safe text\n",
    "# whisper : speech-to-text\n",
    "# tool : for tool usage\n",
    "# llava : Context Window: 4,096 tokens\n",
    "# gemma- : older than gemma2\n",
    "# llama-3.1 : in 'preview' stage\n",
    "models_selected: list[str] = sorted(\n",
    "    md[\"id\"] for md in models_full_list if md[\"active\"] and all(ex not in md[\"id\"] for ex in models_excluded)\n",
    ")\n",
    "models_selected\n",
    "# as of 11/09/2024: ['gemma2-9b-it', 'llama3-70b-8192', 'llama3-8b-8192', 'mixtral-8x7b-32768']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waking up assistant, if needed\n",
    "llm_api_client: Groq = get_llm_api_client_object(api_name=LLM_API_NAME)(api_key=LLM_API_KEY)\n",
    "model_name = \"mixtral-8x7b-32768\"  # for test on 09/09/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_input(\n",
    "    user_prompt: str,\n",
    "    k_base: KBaseTable,\n",
    "    client: Groq,\n",
    "    llm_temp: float,\n",
    "    model_name: str,\n",
    ") -> tuple[str, str]:\n",
    "    # create chat history\n",
    "    chat_history: list[dict[str, str]] = [\n",
    "        {\"role\": \"assistant\", \"content\": WELCOME_MSG.format(user_name=\"John Doe\")},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    # build LLM chat input\n",
    "    messages: list[dict[str, str]] = build_full_llm_chat_input(\n",
    "        user_prompt=user_prompt,\n",
    "        chat_history=chat_history,\n",
    "        k_base=k_base,\n",
    "        retriever_config=retriever_config,\n",
    "    )\n",
    "    context: str = extract_context_from_msg(messages[0][\"content\"])\n",
    "\n",
    "    # send message to LLM and get response\n",
    "    response_raw: Iterable = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model_name,\n",
    "        temperature=llm_temp,\n",
    "        stream=False,\n",
    "    )\n",
    "    txt_response: str = response_raw.choices[0].message.content\n",
    "\n",
    "    return {\"context\": context, \"txt_response\": txt_response, \"full_response\": response_raw}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "entry = ground_truth.iloc[0]\n",
    "user_prompt: str = entry[\"title\"].lower().replace(\":\", \" \").replace(\"\\u00a0\", \" \").strip()\n",
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_config: dict = {\"llm_temp\": LLM_TEMP, \"k_base\": k_base, \"client\": llm_api_client, \"model_name\": model_name}\n",
    "resp_dict = process_user_input(user_prompt=user_prompt, **chat_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp_dict[\"txt_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get text chunk vector from knowledge base\n",
    "hash_doc: str = entry[\"best_doc_hash\"]\n",
    "text_chunk_vec: list[float] = k_base.search().where(f\"hash_doc = '{hash_doc}'\").to_pandas().iloc[0][\"vector\"].tolist()\n",
    "# create embedding vector of response\n",
    "resp_vec: list[float] = emb_model.encode([resp_dict[\"txt_response\"]])[0]\n",
    "# compute similarity\n",
    "emb_model.similarity([text_chunk_vec], [resp_vec]).tolist()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop: compute similarity between expected text chunk and generated LLM response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name = \"John Doe\"\n",
    "\n",
    "pbar = tqdm(ground_truth.iterrows(), total=len(ground_truth))\n",
    "\n",
    "eva_input: dict[str, dict[str, Any]] = {}\n",
    "for _, entry in pbar:\n",
    "    # get has of best doc for given title\n",
    "    hash_doc: str = entry[\"best_doc_hash\"]\n",
    "\n",
    "    eva_input[hash_doc] = {}\n",
    "\n",
    "    # prepare query\n",
    "    user_prompt: str = entry[\"title\"].lower().replace(\":\", \" \").replace(\"\\u00a0\", \" \").strip()\n",
    "    eva_input[hash_doc][\"query_text\"] = user_prompt\n",
    "\n",
    "    # create chat history\n",
    "    chat_history: list[dict[str, str]] = [\n",
    "        {\"role\": \"assistant\", \"content\": WELCOME_MSG.format(user_name=user_name)},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    # build LLM chat input\n",
    "    prompt: list[dict[str, str]] = build_full_llm_chat_input(\n",
    "        user_prompt=user_prompt,\n",
    "        chat_history=chat_history,\n",
    "        k_base=k_base,\n",
    "        retriever_config=retriever_config,\n",
    "    )\n",
    "    eva_input[hash_doc][\"prompt\"] = prompt\n",
    "\n",
    "    # get text chunk vector from knowledge base\n",
    "    text_chunk_vec: list[float] = (\n",
    "        k_base.search().where(f\"hash_doc = '{hash_doc}'\").to_pandas().iloc[0][\"vector\"].tolist()\n",
    "    )\n",
    "    eva_input[hash_doc][\"text_chunk_vec\"] = text_chunk_vec\n",
    "# < 1 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in\n",
    "eva_dict: dict[str, dict[str, Any]]\n",
    "if eva_rag_results.exists():\n",
    "    with open(eva_rag_results) as f:\n",
    "        eva_dict = json.load(f)\n",
    "else:\n",
    "    eva_dict = {}\n",
    "print(f\"Number fo evaluation results: {len(eva_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pbar_update(pbar: tqdm, txt: str) -> None:\n",
    "    pbar.set_description(f\"{txt:<30}\")\n",
    "    pbar.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(eva_input.items(), total=len(eva_input))\n",
    "for hash_doc, data in pbar:\n",
    "    # continue  # for testing\n",
    "\n",
    "    if hash_doc in eva_dict:\n",
    "        continue\n",
    "\n",
    "    # build LLM chat input\n",
    "    prompt: list[dict[str, str]] = data[\"prompt\"]\n",
    "\n",
    "    # get text chunk vector from knowledge base\n",
    "    text_chunk_vec = data[\"text_chunk_vec\"]\n",
    "\n",
    "    # send message to LLM and get response\n",
    "    out: dict = {\"input\": {\"messages\": prompt, \"text_chunk_vec\": text_chunk_vec}, \"llm_rsp\": {}}\n",
    "    for model_name in models_selected:\n",
    "        out[\"llm_rsp\"][model_name] = {}\n",
    "\n",
    "        pbar_update(pbar, f\"'{model_name}': Sleeping...\")\n",
    "        time.sleep(0.5)  # avoid rate limit problems\n",
    "\n",
    "        pbar_update(pbar, f\"'{model_name}': Waiting for response ...\")\n",
    "        response_raw: Iterable = llm_api_client.chat.completions.create(\n",
    "            messages=prompt, model=model_name, temperature=LLM_TEMP, stream=False\n",
    "        )\n",
    "        txt_response: str = response_raw.choices[0].message.content\n",
    "\n",
    "        # ask LLM to process query\n",
    "        pbar_update(pbar, f\"'{model_name}': Saving response ...\")\n",
    "        out[\"llm_rsp\"][model_name][\"txt_response\"] = txt_response\n",
    "        out[\"llm_rsp\"][model_name][\"usage\"] = dict(response_raw.usage)\n",
    "\n",
    "        pbar_update(pbar, f\"'{model_name}': Computing similarity...\")\n",
    "        # create embedding vector of response\n",
    "        resp_vec: list[float] = emb_model.encode([txt_response])[0]\n",
    "        # compute similarity\n",
    "        out[\"llm_rsp\"][model_name][\"similarity\"] = emb_model.similarity([text_chunk_vec], [resp_vec]).tolist()[0][0]\n",
    "\n",
    "    # save results\n",
    "    eva_dict[hash_doc] = out\n",
    "\n",
    "#\n",
    "# < 50 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save full results to json file\n",
    "with open(eva_rag_results, \"w\") as f:\n",
    "    json.dump(eva_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 09/09/2024 Evaluation results\n",
    "# # save just the similarity scores\n",
    "# df = pd.DataFrame().from_dict(eva_dict, orient=\"index\")\n",
    "# df.index.name = \"hash_doc\"\n",
    "# df[\"similarity\"].to_csv(eva_rag_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since: 11/09/2024\n",
    "out = {}\n",
    "for hash_doc, data in eva_dict.items():\n",
    "    out[hash_doc] = {model_name: data[\"llm_rsp\"][model_name][\"similarity\"] for model_name in models_selected}\n",
    "df = pd.DataFrame().from_dict(out, orient=\"index\")\n",
    "# set index name to hash_doc\n",
    "df.index.name = \"hash_doc\"\n",
    "# save\n",
    "df.to_csv(eva_rag_similarity, index=True)\n",
    "\n",
    "out = {}\n",
    "for hash_doc, data in eva_dict.items():\n",
    "    out[hash_doc] = {\n",
    "        model_name: data[\"llm_rsp\"][model_name][\"usage\"][\"completion_time\"] for model_name in models_selected\n",
    "    }\n",
    "df = pd.DataFrame().from_dict(out, orient=\"index\")\n",
    "df.index.name = \"hash_doc\"  # set index name to hash_doc\n",
    "df.to_csv(GROUND_TRUTH_PATH / f\"eva_rag_rsp_time_{ts}.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple models (since 11/09/2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_answer = pd.read_csv(eva_rag_similarity, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean and std for each column\n",
    "stats = pd.DataFrame({\"mean\": sim_answer.mean(), \"std\": sim_answer.std()}).sort_values(by=\"mean\", ascending=False)\n",
    "stats.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_answer.hist(bins=20, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = pd.concat([ground_truth.set_index(\"best_doc_hash\")[\"best_doc_sim\"], sim_answer], axis=1).rename(\n",
    "    columns={\"best_doc_sim\": \"Baseline\"}\n",
    ")\n",
    "merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "axix = merge.plot.kde()\n",
    "# ground_truth[\"best_doc_sim\"].plot.kde(color=\"black\", ls=\"--\")\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.grid(True)\n",
    "plt.title(\"RAG evaluation: Similarity Distribution\")\n",
    "plt.xlim(0.7, 1)\n",
    "plt.legend()  # list(sim_answer.columns) + [\"baseline\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis = merge.plot.box(figsize=(7, 3), showfliers=False)\n",
    "axis.set_ylabel(\"Cosine Similarity\")\n",
    "axis.set_title(\"RAG evaluation: Similarity\")\n",
    "# title x lables by 45 degree\n",
    "# axis.set_xticklabels(axis.get_xticklabels(), rotation=45)\n",
    "# reduce font size of x labels\n",
    "axis.tick_params(axis=\"x\", labelsize=8)\n",
    "axis.grid(True)\n",
    "# save figure\n",
    "plt.savefig(eva_rag_similarity.parent / f\"{eva_rag_similarity.stem}_sim_box.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_answer = pd.read_csv(GROUND_TRUTH_PATH / f\"eva_rag_rsp_time_{ts}.csv\", index_col=0)\n",
    "axis = time_answer.plot.box(figsize=(6, 3), showfliers=False)\n",
    "axis.set_ylabel(\"Response Time (s)\")\n",
    "axis.set_title(\"RAG evaluation: Response Time\")\n",
    "axis.tick_params(axis=\"x\", labelsize=8)\n",
    "axis.grid(True)\n",
    "plt.savefig(eva_rag_similarity.parent / f\"{eva_rag_similarity.stem}_time_box.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single model: mistral (09/09/2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_answer = pd.read_csv(\"data/ground_truth/eva_rag_similarity.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average similarity between response and expected text chunk\n",
    "print(f\"Average similarity: {sim_answer['similarity'].mean():.2f}+-{sim_answer['similarity'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute probability density distribution and kde\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "sim_answer[\"similarity\"].hist(bins=20, density=True)\n",
    "sim_answer[\"similarity\"].plot.kde()\n",
    "# show average and std as transparent vertical band\n",
    "\n",
    "ground_truth[\"best_doc_sim\"].plot.kde()\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.grid(True)\n",
    "plt.title(\"RAG evaluation: Similarity Distribution\")\n",
    "plt.xlim(0.7, 1)\n",
    "plt.legend([\"Generated answer vs expected answer\", \"Query vs expected answer\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = pd.concat([ground_truth.set_index(\"best_doc_hash\")[\"best_doc_sim\"], sim_answer], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 4))\n",
    "axis = merge.plot.box(figsize=(6, 3), showfliers=False)\n",
    "axis.set_ylabel(\"Cosine Similarity\")\n",
    "axis.set_title(\"RAG evaluation\")\n",
    "# change x tick labels to be more readable\n",
    "tick_labels = [\"Query vs Expected answer\\n(Baseline)\", \"Generated answer\\nvs Expected answer\"]\n",
    "axis.set_xticks(ticks=[1, 2], labels=tick_labels)\n",
    "axis.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
