{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of chatbot answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from collections.abc import Iterable\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import lancedb\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "from lancedb.table import Table as KBaseTable\n",
    "from matplotlib import pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.constants import (\n",
    "    GROUND_TRUTH_FILE,\n",
    "    GROUND_TRUTH_PATH,\n",
    "    LANCEDB_URI,\n",
    "    REPO_PATH,\n",
    "    get_rag_config,\n",
    ")\n",
    "from src.llm_api import (\n",
    "    build_full_llm_chat_input,\n",
    "    get_llm_api_client_object,\n",
    "    get_preferred_model,\n",
    ")\n",
    "from src.prompt_building import WELCOME_MSG, extract_context_from_msg\n",
    "\n",
    "# to ignore warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name: str = get_rag_config()[\"knowledge_base\"][\"table_name\"]\n",
    "emb_config: dict = get_rag_config()[\"embeddings\"]\n",
    "\n",
    "# LLM Parameters\n",
    "# -----------------------------\n",
    "llm_api_config: dict[str, Any] = get_rag_config()[\"llm\"]\n",
    "LLM_TEMP: float = llm_api_config[\"settings\"][\"model_temp\"]\n",
    "LLM_API_NAME: str = llm_api_config[\"settings\"][\"api_name\"]\n",
    "LLM_API_CONFIG: dict[str, Any] = llm_api_config[\"api\"][LLM_API_NAME]\n",
    "LLM_API_KEY_NAME: str = LLM_API_CONFIG[\"key_name\"]\n",
    "LLM_API_KEY_URL: str = LLM_API_CONFIG[\"key_url\"]\n",
    "\n",
    "\n",
    "# Secrets\n",
    "# -----------------------------\n",
    "load_dotenv(REPO_PATH)\n",
    "LLM_API_KEY: str = os.getenv(LLM_API_KEY_NAME)\n",
    "\n",
    "# Paths\n",
    "eva_rag_results: Path = GROUND_TRUTH_PATH / \"eva_rag_results.json\"\n",
    "eva_rag_similarity: Path = GROUND_TRUTH_PATH / \"eva_rag_similarity.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db: lancedb.db.DBConnection = lancedb.connect(uri=LANCEDB_URI)\n",
    "print(f\"List of all tables in the LanceDB database: {db.table_names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_base: lancedb.table.Table = db.open_table(table_name)\n",
    "print(f\"Number of entries in the table '{table_name}': {k_base.count_rows()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure similarity between titles\n",
    "emb_model = SentenceTransformer(\n",
    "    emb_config[\"model_name\"],\n",
    "    device=emb_config[\"device\"],\n",
    "    similarity_fn_name=emb_config[\"metric\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take ground truth entries with where text chunk of the title has at least a cosine similarity of 0.8\n",
    "ground_truth = pd.read_csv(GROUND_TRUTH_FILE).loc[lambda df: df[\"best_doc_sim\"] > 0.8]\n",
    "print(f\"Number of entries in the ground truth: {len(ground_truth)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground Truth: Average Similarity of text chunk to its title\n",
    "print(f\"Average similarity: {ground_truth['best_doc_sim'].mean():.2f}+-{ground_truth['best_doc_sim'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_llm(api_key: str, api_name: str, api_config: dict, model_name: str = \"\") -> tuple[Groq, str]:\n",
    "    # Setup Model Name\n",
    "    if not model_name:\n",
    "        models_url: str = api_config.get(\"models\", {}).get(\"url\", \"\")\n",
    "        ranked_models: list[str] = api_config.get(\"models\", {}).get(\"ranked\", [])\n",
    "        model_name = get_preferred_model(api_key=api_key, models_url=models_url, ranked_models=ranked_models)\n",
    "\n",
    "    # Setup LLM API Client\n",
    "    llm_api_client = get_llm_api_client_object(api_name=api_name)\n",
    "    return llm_api_client(api_key=api_key), model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waking up assistant, if needed\n",
    "llm_api_client: Groq\n",
    "llm_api_client, model_name = connect_to_llm(api_key=LLM_API_KEY, api_name=LLM_API_NAME, api_config=LLM_API_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_config: dict = {\"llm_temp\": LLM_TEMP, \"k_base\": k_base, \"client\": llm_api_client, \"model_name\": model_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_input(\n",
    "    user_prompt: str,\n",
    "    k_base: KBaseTable,\n",
    "    client: Groq,\n",
    "    llm_temp: float,\n",
    "    model_name: str,\n",
    ") -> tuple[str, str]:\n",
    "    # create chat history\n",
    "    chat_history: list[dict[str, str]] = [\n",
    "        {\"role\": \"assistant\", \"content\": WELCOME_MSG.format(user_name=\"John Doe\")},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    # build LLM chat input\n",
    "    messages: list[dict[str, str]] = build_full_llm_chat_input(user_prompt, chat_history, k_base)\n",
    "    context: str = extract_context_from_msg(messages[0][\"content\"])\n",
    "\n",
    "    # send message to LLM and get response\n",
    "    response_raw: Iterable = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model_name,\n",
    "        temperature=llm_temp,\n",
    "        stream=False,\n",
    "    )\n",
    "    txt_response: str = response_raw.choices[0].message.content\n",
    "\n",
    "    return {\"context\": context, \"txt_response\": txt_response, \"full_response\": response_raw}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "entry = ground_truth.iloc[0]\n",
    "query_text: str = entry[\"title\"].lower().replace(\":\", \" \").replace(\"\\u00a0\", \" \").strip()\n",
    "print(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_dict = process_user_input(user_prompt=query_text, **chat_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp_dict[\"txt_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get text chunk vector from knowledge base\n",
    "hash_doc: str = entry[\"best_doc_hash\"]\n",
    "text_chunk_vec: list[float] = k_base.search().where(f\"hash_doc = '{hash_doc}'\").to_pandas().iloc[0][\"vector\"].tolist()\n",
    "# create embedding vector of response\n",
    "resp_vec: list[float] = emb_model.encode([resp_dict[\"txt_response\"]])[0]\n",
    "# compute similarity\n",
    "emb_model.similarity([text_chunk_vec], [resp_vec]).tolist()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute similarity between expected text chunk and created response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in\n",
    "with open(eva_rag_results) as f:\n",
    "    eva_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eva_dict: dict[str, dict[str, str | float]] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(ground_truth.iterrows(), total=len(ground_truth))\n",
    "for _, entry in pbar:\n",
    "    continue  # for testing\n",
    "\n",
    "    # get has of best doc for given title\n",
    "    hash_doc: str = entry[\"best_doc_hash\"]\n",
    "    if hash_doc in eva_dict:\n",
    "        continue\n",
    "\n",
    "    pbar.set_description(f\"{'sleeping...':<40}\")\n",
    "    pbar.refresh()\n",
    "    time.sleep(0.5)  # avoid rate limit problems\n",
    "\n",
    "    # if len(eva_dict) >= 2:\n",
    "    #     break\n",
    "\n",
    "    out: dict[str, str | float] = {}\n",
    "\n",
    "    # prepare query\n",
    "    title: str = entry[\"title\"]\n",
    "    query_text: str = title.lower().replace(\":\", \" \").replace(\"\\u00a0\", \" \").strip()\n",
    "    out[\"query_text\"] = query_text\n",
    "\n",
    "    # ask LLM to process query\n",
    "    pbar.set_description(f\"{\"Processing query...\":<40}\")\n",
    "    pbar.refresh()\n",
    "    resp_dict = process_user_input(user_prompt=query_text, **chat_config)\n",
    "    txt_response: str = resp_dict[\"txt_response\"]\n",
    "    out[\"txt_response\"] = txt_response\n",
    "    out[\"context\"] = resp_dict[\"context\"]\n",
    "\n",
    "    pbar.set_description(f\"{\"Computing similarity...\":<40}\")\n",
    "    pbar.refresh()\n",
    "    # get text chunk vector from knowledge base\n",
    "    text_chunk_vec: list[float] = (\n",
    "        k_base.search().where(f\"hash_doc = '{hash_doc}'\").to_pandas().iloc[0][\"vector\"].tolist()\n",
    "    )\n",
    "    # create embedding vector of response\n",
    "    resp_vec: list[float] = emb_model.encode([txt_response])[0]\n",
    "    # compute similarity\n",
    "    out[\"similarity\"] = emb_model.similarity([text_chunk_vec], [resp_vec]).tolist()[0][0]\n",
    "\n",
    "    # save results\n",
    "    eva_dict[hash_doc] = out\n",
    "\n",
    "#  77/77 [45:17<00:00, 35.29s/it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save results to json file\n",
    "# with open(eva_rag_results, \"w\") as f:\n",
    "#     json.dump(eva_dict, f, indent=4)\n",
    "\n",
    "# # save just the similarity scores\n",
    "# df = pd.DataFrame().from_dict(eva_dict, orient=\"index\")\n",
    "# df.index.name = \"hash_doc\"\n",
    "# df[\"similarity\"].to_csv(eva_rag_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_answer = pd.read_csv(eva_rag_similarity, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average similarity between response and expected text chunk\n",
    "print(f\"Average similarity: {sim_answer['similarity'].mean():.2f}+-{sim_answer['similarity'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute probability density distribution and kde\n",
    "plt.figure(figsize=(15, 4))\n",
    "sim_answer[\"similarity\"].hist(bins=20, density=True)\n",
    "sim_answer[\"similarity\"].plot.kde()\n",
    "# show average and std as transparent vertical band\n",
    "\n",
    "ground_truth[\"best_doc_sim\"].plot.kde()\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.grid(True)\n",
    "plt.title(\"RAG evaluation: Similarity Distribution\")\n",
    "plt.xlim(0.7, 1)\n",
    "plt.legend([\"Generated answer vs expected answer\", \"Query vs expected answer\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = pd.concat([ground_truth.set_index(\"best_doc_hash\")[\"best_doc_sim\"], sim_answer], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 4))\n",
    "axis = merge.plot.box(figsize=(6, 3), showfliers=False)\n",
    "axis.set_ylabel(\"Cosine Similarity\")\n",
    "axis.set_title(\"RAG evaluation\")\n",
    "# change x tick labels to be more readable\n",
    "tick_labels = [\"Query vs Expected answer\\n(Baseline)\", \"Generated answer\\nvs Expected answer\"]\n",
    "axis.set_xticks(ticks=[1, 2], labels=tick_labels)\n",
    "axis.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
