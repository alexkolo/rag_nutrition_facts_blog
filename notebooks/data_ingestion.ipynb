{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from lancedb.embeddings import get_registry\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceEmbedder:\n",
    "    def __init__(self, model_name: str, api_key: str):\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def embed(self, texts: list[str]) -> list[list[float]]:\n",
    "        api_url = f\"https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/{self.model_name}\"\n",
    "        response = requests.post(\n",
    "            url=api_url,\n",
    "            headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n",
    "            json={\n",
    "                \"inputs\": texts,\n",
    "                \"options\": {\"wait_for_model\": True, \"use_cache\": True},\n",
    "            },\n",
    "        )\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "data_path = Path(\".\").resolve().parent / \"data\"\n",
    "data_path.is_dir()  # fails if it doesn't exist\n",
    "blog_posts_root: Path = data_path / \"blog_posts\"\n",
    "post_path_json: Path = blog_posts_root / \"json\"\n",
    "post_path_json.is_dir()  # fails if it doesn't exist\n",
    "\n",
    "# secrets\n",
    "api_key = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Embedding Models\n",
    "- Original Models : https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "> The `all-mpnet-base-v2` model provides the best quality, while `all-MiniLM-L6-v2` is 5 times faster and still offers good quality\n",
    "\n",
    "`multi-qa-MiniLM-L6-cos-v1`  (80MB) : \"tuned for semantic search: Given a query/question, it can find relevant passages. It was trained on a large and diverse set of (question, answer) pairs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"multi-qa-MiniLM-L6-cos-v1\"\n",
    "test_docs = [\"Hello world\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model locally\n",
    "\n",
    "- big package: https://stackoverflow.com/questions/77205123/how-do-i-slim-down-sberts-sentencer-transformer-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings01 = model.encode(test_docs).tolist()\n",
    "# embeddings01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model from HuggingFace API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = HuggingFaceEmbedder(model_name=model_name, api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings02 = embedder.embed(test_docs)\n",
    "# embeddings02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare embeddings\n",
    "(np.array(embeddings01) / np.array(embeddings02)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LanceDB Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registry = get_registry().get(\"sentence-transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_registry.create(name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ndims()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single JSON file\n",
    "\n",
    "emb_model = SentenceTransformer(model_name)\n",
    "\n",
    "\n",
    "def emb_funct(text: list[str]) -> list[list[float]]:\n",
    "    return emb_model.encode(text).tolist()\n",
    "\n",
    "\n",
    "def process_json_file(file_path: Path, emb_funct) -> pd.DataFrame:\n",
    "    with open(file_path) as f:\n",
    "        data: dict = json.load(f)\n",
    "\n",
    "    # Extract the text data\n",
    "    paragraphs: list[str] = data.get(\"paragraphs\", [])\n",
    "    key_takeaways: list[str] = data.get(\"key_takeaways\", [])\n",
    "    combined_text: list[str] = paragraphs + key_takeaways\n",
    "\n",
    "    # Create embeddings for each text chunk\n",
    "    embeddings: list[list[float]] = emb_funct(combined_text)\n",
    "\n",
    "    # Prepare a DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"url\": [data.get(\"url\")] * len(combined_text),\n",
    "            \"title\": [data.get(\"title\")] * len(combined_text),\n",
    "            \"text\": combined_text,\n",
    "            \"embedding\": embeddings,\n",
    "            \"blog_tags\": [\" \".join(data.get(\"blog_tags\"))] * len(combined_text),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all JSON files and process them\n",
    "files: list[Path] = list(post_path_json.glob(\"*.json\"))\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for json_file in files[:1]:\n",
    "    df = process_json_file(file_path=json_file, emb_funct=emb_funct)\n",
    "    all_data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context(\"display.max_colwidth\", None):\n",
    "#     display(df.iloc[[0]].style.set_properties(**{\"text-align\": \"left\"}))\n",
    "df.iloc[[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
