{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion via LanceDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import lancedb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from lancedb.embeddings import get_registry\n",
    "from lancedb.embeddings.sentence_transformers import SentenceTransformerEmbeddings\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# isort: off\n",
    "sys.path.append(\"..\")  # include repository-root to load modules from src folder\n",
    "from src.constants import (  # noqa: E402\n",
    "    LANCEDB_URI,\n",
    "    POST_JSON_PATH,\n",
    "    REPO_PATH,\n",
    "    get_rag_config,\n",
    ")\n",
    "from src.embeddings import HuggingFaceEmbedder, EmbeddingFunction  # noqa: E402\n",
    "from src.chunking import recursive_text_splitter  # noqa: E402\n",
    "\n",
    "# ignore some warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "POST_JSON_PATH.is_dir()  # fails if it doesn't exist\n",
    "LANCEDB_URI.is_dir()  # fails if it doesn't exist\n",
    "\n",
    "\n",
    "# Embeddings\n",
    "emb_model_name: str = get_rag_config()[\"embeddings\"][\"model_name\"]\n",
    "device: str = get_rag_config()[\"hardware\"][\"device\"]\n",
    "\n",
    "# secrets\n",
    "load_dotenv(REPO_PATH)\n",
    "api_key = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Embedding Models\n",
    "- Original Models : https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "> The `all-mpnet-base-v2` model provides the best quality, while `all-MiniLM-L6-v2` is 5 times faster and still offers good quality\n",
    "\n",
    "`multi-qa-MiniLM-L6-cos-v1`  (80MB) : \"tuned for semantic search: Given a query/question, it can find relevant passages. It was trained on a large and diverse set of (question, answer) pairs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"multi-qa-MiniLM-L6-cos-v1\"\n",
    "test_docs = [\"Hello world\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model locally\n",
    "\n",
    "- big package: https://stackoverflow.com/questions/77205123/how-do-i-slim-down-sberts-sentencer-transformer-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings01 = model.encode(test_docs).tolist()\n",
    "# embeddings01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model from HuggingFace API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = HuggingFaceEmbedder(model_name=model_name, api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow since it's an API call\n",
    "embeddings02 = embedder.embed(test_docs)\n",
    "# embeddings02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare embeddings\n",
    "(1 - np.array(embeddings01) / np.array(embeddings02)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LanceDB Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registry = get_registry().get(\"sentence-transformers\")\n",
    "model = model_registry.create(name=model_name)\n",
    "model.ndims()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old: Function to process a single JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single JSON file\n",
    "\n",
    "\n",
    "def process_json_file(file_path: Path, emb_func: EmbeddingFunction) -> pd.DataFrame:\n",
    "    with open(file_path) as f:\n",
    "        data: dict = json.load(f)\n",
    "\n",
    "    # Extract the text data\n",
    "    paragraphs: list[str] = data.get(\"paragraphs\", [])\n",
    "    key_takeaways: list[str] = data.get(\"key_takeaways\", [])\n",
    "    combined_text: list[str] = paragraphs + key_takeaways\n",
    "\n",
    "    # Create embeddings for each text chunk\n",
    "    embeddings: list[list[float]] = emb_func(combined_text)\n",
    "\n",
    "    # Prepare a DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"url\": [data.get(\"url\")] * len(combined_text),\n",
    "            \"title\": [data.get(\"title\")] * len(combined_text),\n",
    "            \"text\": combined_text,\n",
    "            \"embedding\": embeddings,\n",
    "            \"blog_tags\": [\" \".join(set(data.get(\"blog_tags\")))] * len(combined_text),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using local model\n",
    "emb_model = SentenceTransformer(model_name)\n",
    "\n",
    "\n",
    "def emb_func(text: list[str]) -> list[list[float]]:\n",
    "    return emb_model.encode(text).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all JSON files and process them\n",
    "files: list[Path] = list(POST_JSON_PATH.glob(\"*.json\"))\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for json_file in files[:1]:\n",
    "    df = process_json_file(file_path=json_file, emb_func=emb_func)\n",
    "    all_data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context(\"display.max_colwidth\", None):\n",
    "#     display(df.iloc[[0]].style.set_properties(**{\"text-align\": \"left\"}))\n",
    "df.iloc[[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean pharagraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = SentenceTransformer(emb_model_name, device=device)\n",
    "\n",
    "# Get the tokenizer from the model\n",
    "tokenizer = emb_model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all JSON files and process them\n",
    "files: list[Path] = list(POST_JSON_PATH.glob(\"*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/alex/repos/rag_nutrition_facts_blog/notebooks/../data/blog_posts/json/eliminate-90-percent-heart-disease-risk.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_has_only_questions(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the input string contains question marks but not periods or exclamation marks.\n",
    "    (aka text consists of only questions but no information)\n",
    "    \"\"\"\n",
    "    return \"?\" in text and \".\" not in text and \"!\" not in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3816603086.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    [len(para) from para in paragraphs]\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "json_file = files[0]\n",
    "with open(json_file) as f:\n",
    "    doc: dict = json.load(f)\n",
    "paragraphs: list[str] = doc[\"paragraphs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[131, 69, 109, 69, 84, 93, 59, 37, 28, 8]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(tokenizer.tokenize(para)) for para in paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False, False, False, False, False, False, False, False]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[text_has_only_questions(para) for para in paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.732824427480916,\n",
       " 4.579710144927536,\n",
       " 4.926605504587156,\n",
       " 5.318840579710145,\n",
       " 4.785714285714286,\n",
       " 5.032258064516129,\n",
       " 5.084745762711864,\n",
       " 4.162162162162162,\n",
       " 4.142857142857143,\n",
       " 5.25]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(para) / len(tokenizer.tokenize(para)) for para in paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 131 tokens: split needed. New chunks: 2\n",
      "1: 69 tokens: split needed. New chunks: 1\n",
      "2: 109 tokens: split needed. New chunks: 2\n",
      "3: 69 tokens: split needed. New chunks: 1\n",
      "4: 84 tokens: split needed. New chunks: 1\n",
      "5: 93 tokens: split needed. New chunks: 2\n",
      "6: 59 tokens: split needed. New chunks: 1\n",
      "7: 37 tokens: split needed. New chunks: 1\n",
      "8: 28 tokens: split needed. New chunks: 1\n",
      "Original: 10 -> New: 13\n"
     ]
    }
   ],
   "source": [
    "n_token_max: int = get_rag_config()[\"embeddings\"][\"n_token_max\"]\n",
    "n_char_max: int = n_token_max * 4\n",
    "overlap: int = int(n_char_max * 0.1)\n",
    "paragraphs_new: list[str] = []\n",
    "for i, para in enumerate(paragraphs):\n",
    "    if text_has_only_questions(para):\n",
    "        continue\n",
    "    n_token: int = len(tokenizer.tokenize(para))\n",
    "    if n_token > n_token_max:\n",
    "        para_chunks: list[str] = recursive_text_splitter(para, n_char_max, overlap)\n",
    "        print(f\"{i}: {n_token} tokens: split needed. New chunks: {len(para_chunks)}\")\n",
    "        paragraphs_new.extend(para_chunks)\n",
    "    else:\n",
    "        paragraphs_new.append(para)\n",
    "\n",
    "print(f\"Original: {len(paragraphs)} -> New: {len(paragraphs_new)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' myths and dogmas die hard. Researchers creating a new body of knowledge for prevention and control of heart disease had to disprove a bunch of doozies. For example, we used to think that heart disease, high cholesterol, and high blood pressure were just inevitable consequences of aging. All these are now bygone notions, refuted by massive data. Other long-standing myths and dogmas about our number one killer epidemic persist, however. For example, many still'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' persist, however. For example, many still think that major risk factors, like cholesterol, account for a minority of risk, and that many people have heart attacks with no risk factors at all.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs_new[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"bla adfasb ? absada ?\"\n",
    "b = \"bla adfasb ? absada .\"\n",
    "\n",
    "\n",
    "def check(q: str) -> bool:\n",
    "    # check if q has only question marks but not periods nor exclamation marks\n",
    "    return all(q.count(char) == 1 for char in \"?!.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data set to ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file list of JSON files\n",
    "files: list[Path] = list(POST_JSON_PATH.glob(\"*.json\"))\n",
    "print(f\"{len(files)} JSON files are in: {POST_JSON_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_data: list[dict[str, str]] = []\n",
    "\n",
    "for json_file in files[:2]:\n",
    "    with open(json_file) as f:\n",
    "        doc: dict = json.load(f)\n",
    "    paragraphs: list[str] = doc[\"paragraphs\"]\n",
    "    title: str = doc[\"title\"]\n",
    "    url: str = doc[\"url\"]\n",
    "    blog_tags: str = \" \".join(\n",
    "        set(doc[\"blog_tags\"])\n",
    "    )  # remove duplicates and join with space\n",
    "    test_table_data.extend(\n",
    "        [\n",
    "            {\"text\": para, \"title\": title, \"url\": url, \"blog_tags\": blog_tags}\n",
    "            for para in paragraphs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# print number of entries\n",
    "print(f\"{len(test_table_data)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 01 : Simple: just text + vector\n",
    "\n",
    "- following: \n",
    "    - https://lancedb.github.io/lancedb/embeddings/embedding_functions/\n",
    "    - https://lancedb.github.io/lancedb/embeddings/available_embedding_models/text_embedding_functions/sentence_transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embedding function\n",
    "emb_model: SentenceTransformerEmbeddings = (\n",
    "    get_registry().get(\"sentence-transformers\").create(name=emb_model_name)\n",
    ")\n",
    "n_dim_vec = emb_model.ndims()\n",
    "\n",
    "\n",
    "# Define the data model or schema\n",
    "class DataModel(LanceModel):\n",
    "    vector: Vector(dim=n_dim_vec) = emb_model.VectorField()\n",
    "    text: str = emb_model.SourceField()\n",
    "    title: str\n",
    "    url: str\n",
    "    blog_tags: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create/connect to the database\n",
    "db: lancedb.db.DBConnection = lancedb.connect(uri=LANCEDB_URI)\n",
    "\n",
    "# create table via schema, which creates embeddings for the text column stored in the vector column\n",
    "table: lancedb.table.Table = db.create_table(\n",
    "    \"table01\", schema=DataModel, mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# add data to the table, which creates embeddings for the text column stored in the vector column\n",
    "table.add(data=test_table_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing table content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test input\n",
    "table.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test simple trivial query\n",
    "query = \"How to reduce Heart Disease Risk\"\n",
    "response: list[DataModel] = table.search(query).limit(5).to_pydantic(DataModel)\n",
    "\n",
    "# unique URLs\n",
    "urls: set = {actual.url for actual in response}\n",
    "print(f\"{len(urls)} unique URL(s)\")\n",
    "\n",
    "# unique Titles\n",
    "titles: set = {actual.title for actual in response}\n",
    "print(f\"{len(titles)} unique Title(s)\")\n",
    "\n",
    "print(f\"{len(response)} results for: '{query}'\")\n",
    "for i, actual in enumerate(response):\n",
    "    print(f\"\\t{i}. {actual.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = table.search(query).to_pydantic(DataModel)\n",
    "print(f\"{len(response)} results for: {query}\")\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
