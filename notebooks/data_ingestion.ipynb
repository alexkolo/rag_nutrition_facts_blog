{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion via LanceDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import lancedb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from lancedb.embeddings import get_registry\n",
    "from lancedb.embeddings.sentence_transformers import SentenceTransformerEmbeddings\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.table import Table\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from src.chunking import recursive_text_splitter, text_has_only_questions\n",
    "from src.constants import LANCEDB_URI, POST_JSON_PATH, REPO_PATH, get_rag_config\n",
    "from src.embeddings import EmbeddingFunction, HuggingFaceEmbedder\n",
    "\n",
    "# ignore some warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "POST_JSON_PATH.is_dir()  # fails if it doesn't exist\n",
    "LANCEDB_URI.is_dir()  # fails if it doesn't exist\n",
    "\n",
    "\n",
    "# Embeddings\n",
    "emb_model_name: str = get_rag_config()[\"embeddings\"][\"model_name\"]\n",
    "device: str = get_rag_config()[\"embeddings\"][\"device\"]\n",
    "\n",
    "# secrets\n",
    "load_dotenv(REPO_PATH)\n",
    "api_key = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Embedding Models\n",
    "- Original Models : https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "> The `all-mpnet-base-v2` model provides the best quality, while `all-MiniLM-L6-v2` is 5 times faster and still offers good quality\n",
    "\n",
    "`multi-qa-MiniLM-L6-cos-v1`  (80MB) : \"tuned for semantic search: Given a query/question, it can find relevant passages. It was trained on a large and diverse set of (question, answer) pairs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"multi-qa-MiniLM-L6-cos-v1\"\n",
    "test_docs = [\"Hello world\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model locally\n",
    "\n",
    "- big package: https://stackoverflow.com/questions/77205123/how-do-i-slim-down-sberts-sentencer-transformer-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings01 = model.encode(test_docs).tolist()\n",
    "# embeddings01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model from HuggingFace API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = HuggingFaceEmbedder(model_name=model_name, api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow since it's an API call\n",
    "embeddings02 = embedder.embed(test_docs)\n",
    "# embeddings02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare embeddings\n",
    "(1 - np.array(embeddings01) / np.array(embeddings02)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LanceDB Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registry = get_registry().get(\"sentence-transformers\")\n",
    "model = model_registry.create(name=model_name)\n",
    "model.ndims()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old: Function to process a single JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single JSON file\n",
    "\n",
    "\n",
    "def process_json_file(file_path: Path, emb_func: EmbeddingFunction) -> pd.DataFrame:\n",
    "    with open(file_path) as f:\n",
    "        data: dict = json.load(f)\n",
    "\n",
    "    # Extract the text data\n",
    "    paragraphs: list[str] = data.get(\"paragraphs\", [])\n",
    "    key_takeaways: list[str] = data.get(\"key_takeaways\", [])\n",
    "    combined_text: list[str] = paragraphs + key_takeaways\n",
    "\n",
    "    # Create embeddings for each text chunk\n",
    "    embeddings: list[list[float]] = emb_func(combined_text)\n",
    "\n",
    "    # Prepare a DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"url\": [data.get(\"url\")] * len(combined_text),\n",
    "            \"title\": [data.get(\"title\")] * len(combined_text),\n",
    "            \"text\": combined_text,\n",
    "            \"embedding\": embeddings,\n",
    "            \"blog_tags\": [\" \".join(set(data.get(\"blog_tags\")))] * len(combined_text),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using local model\n",
    "emb_model = SentenceTransformer(model_name)\n",
    "\n",
    "\n",
    "def emb_func(text: list[str]) -> list[list[float]]:\n",
    "    return emb_model.encode(text).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all JSON files and process them\n",
    "files: list[Path] = list(POST_JSON_PATH.glob(\"*.json\"))\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for json_file in files[:1]:\n",
    "    df = process_json_file(file_path=json_file, emb_func=emb_func)\n",
    "    all_data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context(\"display.max_colwidth\", None):\n",
    "#     display(df.iloc[[0]].style.set_properties(**{\"text-align\": \"left\"}))\n",
    "df.iloc[[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk and filter pharagraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = SentenceTransformer(emb_model_name, device=device)\n",
    "\n",
    "# Get the tokenizer from the model\n",
    "tokenizer = emb_model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all JSON files and process them\n",
    "files: list[Path] = list(POST_JSON_PATH.glob(\"*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = files[0]\n",
    "with open(json_file) as f:\n",
    "    doc: dict = json.load(f)\n",
    "paragraphs: list[str] = doc[\"paragraphs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(tokenizer.tokenize(para)) for para in paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[text_has_only_questions(para) for para in paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(para) / len(tokenizer.tokenize(para)) for para in paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_token_max: int = get_rag_config()[\"embeddings\"][\"n_token_max\"]\n",
    "n_char_max: int = n_token_max * 4\n",
    "overlap: int = int(n_char_max * 0.1)\n",
    "paragraphs_new: list[str] = []\n",
    "for i, para in enumerate(paragraphs):\n",
    "    if text_has_only_questions(para):\n",
    "        continue\n",
    "    n_token: int = len(tokenizer.tokenize(para))\n",
    "    if n_token > n_token_max:\n",
    "        para_chunks: list[str] = recursive_text_splitter(para, n_char_max, overlap)\n",
    "        print(f\"{i}: {n_token} tokens: split needed. New chunks: {len(para_chunks)}\")\n",
    "        paragraphs_new.extend(para_chunks)\n",
    "    else:\n",
    "        paragraphs_new.append(para)\n",
    "\n",
    "print(f\"Original: {len(paragraphs)} -> New: {len(paragraphs_new)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_new[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data set to ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file list of JSON files\n",
    "files: list[Path] = list(POST_JSON_PATH.glob(\"*.json\"))\n",
    "print(f\"{len(files)} JSON files are in: {POST_JSON_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_data: list[dict[str, str]] = []\n",
    "\n",
    "for json_file in files[:2]:\n",
    "    with open(json_file) as f:\n",
    "        doc: dict = json.load(f)\n",
    "    paragraphs: list[str] = doc[\"paragraphs\"]\n",
    "    title: str = doc[\"title\"]\n",
    "    url: str = doc[\"url\"]\n",
    "    blog_tags: str = \" \".join(set(doc[\"blog_tags\"]))  # remove duplicates and join with space\n",
    "    test_table_data.extend([{\"text\": para, \"title\": title, \"url\": url, \"blog_tags\": blog_tags} for para in paragraphs])\n",
    "\n",
    "# print number of entries\n",
    "print(f\"{len(test_table_data)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 01 : Simple: just text + vector\n",
    "\n",
    "- following: \n",
    "    - https://lancedb.github.io/lancedb/embeddings/embedding_functions/\n",
    "    - https://lancedb.github.io/lancedb/embeddings/available_embedding_models/text_embedding_functions/sentence_transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embedding function\n",
    "emb_model: SentenceTransformerEmbeddings = get_registry().get(\"sentence-transformers\").create(name=emb_model_name)\n",
    "n_dim_vec = emb_model.ndims()\n",
    "\n",
    "\n",
    "# Define the data model or schema\n",
    "class DataModel(LanceModel):\n",
    "    vector: Vector(dim=n_dim_vec) = emb_model.VectorField()\n",
    "    text: str = emb_model.SourceField()\n",
    "    title: str\n",
    "    url: str\n",
    "    blog_tags: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create/connect to the database\n",
    "db: lancedb.db.DBConnection = lancedb.connect(uri=LANCEDB_URI)\n",
    "\n",
    "# create table via schema, which creates embeddings for the text column stored in the vector column\n",
    "table: lancedb.table.Table = db.create_table(\"table01\", schema=DataModel, mode=\"overwrite\")\n",
    "\n",
    "# add data to the table, which creates embeddings for the text column stored in the vector column\n",
    "table.add(data=test_table_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing table content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test input\n",
    "table.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test simple trivial query\n",
    "query = \"How to reduce Heart Disease Risk\"\n",
    "response: list[DataModel] = table.search(query).limit(5).to_pydantic(DataModel)\n",
    "\n",
    "# unique URLs\n",
    "urls: set = {actual.url for actual in response}\n",
    "print(f\"{len(urls)} unique URL(s)\")\n",
    "\n",
    "# unique Titles\n",
    "titles: set = {actual.title for actual in response}\n",
    "print(f\"{len(titles)} unique Title(s)\")\n",
    "\n",
    "print(f\"{len(response)} results for: '{query}'\")\n",
    "for i, actual in enumerate(response):\n",
    "    print(f\"\\t{i}. {actual.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = table.search(query).to_pydantic(DataModel)\n",
    "print(f\"{len(response)} results for: {query}\")\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/home/alex/repos/rag_nutrition_facts_blog/data/blog_posts/json/why-doctors-should-not-encourage-breast-self-exams.json\"\n",
    "with open(file) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "paragraphs: list[str] = data[\"paragraphs\"]\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tags: list[str] = data[\"raw_tags\"]\n",
    "tag_set: set[str] = {\n",
    "    tag for tag_str in raw_tags if tag_str.startswith(\"tag-\") for tag in tag_str.split(\"-\")[1:] if len(tag) > 3\n",
    "}\n",
    "tag_set\n",
    "# tags: str = \", \".join(sorted(set(tag_list)))\n",
    "# tags.split(\", \")  # .replace(\", \", \" \")\n",
    "# tags_list = set(tags.replace(\", \", \" \").split())\n",
    "# # remove string that have 3 characters or less\n",
    "# tags_list = [tag for tag in tags_list if len(tag) > 3]\n",
    "# tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACEMENTS: dict[str, str] = {\n",
    "    \"-\": \" \",\n",
    "    \".\": \" \",\n",
    "    \",\": \" \",\n",
    "    \"!\": \" \",\n",
    "    \"?\": \" \",\n",
    "    \"'\": \" \",\n",
    "    '\"': \" \",\n",
    "    \"(\": \" \",\n",
    "    \")\": \" \",\n",
    "    \"[\": \" \",\n",
    "    \"]\": \" \",\n",
    "    \"\\\\\": \" \",\n",
    "    \":\": \" \",\n",
    "    \";\": \" \",\n",
    "    \"*\": \" \",\n",
    "    \"&\": \" \",\n",
    "    \"%\": \" \",\n",
    "    \"^\": \" \",\n",
    "    \"~\": \" \",\n",
    "    \"#\": \" \",\n",
    "    \"$\": \" \",\n",
    "    \"@\": \" \",\n",
    "    \"`\": \" \",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata_ind: list[dict[str, str]] = {}\n",
    "\n",
    "tags_doc = [\n",
    "    list(set(chunk.lower().translate(str.maketrans(REPLACEMENTS)).split()) & set(tag_set))\n",
    "    for rank, chunk in enumerate(paragraphs)\n",
    "]\n",
    "tags_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load database\n",
    "# Testing\n",
    "db = lancedb.connect(uri=LANCEDB_URI)\n",
    "print(f\"List of all tables in the LanceDB database: {db.table_names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table01: Table = db.open_table(\"table_simple01\")\n",
    "print(f\"Number of entries: {table01.count_rows()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_counts01 = [len(text.as_py()) for text in table01.to_arrow()[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "table03 = db.open_table(\"table_simple03\")\n",
    "char_counts03 = [len(text.as_py()) for text in table03.to_arrow()[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "table05 = db.open_table(\"table_simple05\")\n",
    "char_counts05 = [len(text.as_py()) for text in table05.to_arrow()[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "table04 = db.open_table(\"table_simple04\")\n",
    "char_counts04 = [len(text.as_py()) for text in table04.to_arrow()[\"text\"]]\n",
    "# table04.to_arrow()[\"n_char_doc\"].as_py()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table04.to_arrow()\n",
    "first_row = {col: table[col][0].as_py() for col in table.column_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(first_row[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(next(iter(table04.to_arrow()[\"text\"])).as_py())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table04.search().limit(5).to_pandas()[\"text\"]\n",
    "table04.search().limit(100).to_arrow()[\"text\"].to_pylist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "plt.hist(char_counts01[:], bins=300, edgecolor=\"blue\", cumulative=False, density=True, facecolor=\"none\")\n",
    "plt.hist(char_counts03[:], bins=300, edgecolor=\"orange\", cumulative=False, density=True, facecolor=\"none\")\n",
    "# plt.hist(char_counts04[:], bins=300, edgecolor=\"green\", cumulative=False, density=True, facecolor=\"none\")\n",
    "plt.hist(char_counts05[:], bins=300, edgecolor=\"cyan\", cumulative=False, density=True, facecolor=\"none\")\n",
    "# plt.hist(\n",
    "#     table04.to_arrow()[\"n_char_doc\"].to_pandas(),\n",
    "#     bins=300,\n",
    "#     edgecolor=\"red\",\n",
    "#     cumulative=False,\n",
    "#     density=True,\n",
    "#     facecolor=\"none\",\n",
    "# )\n",
    "plt.title(\"Histogram of Character Counts in Text Entries\")\n",
    "plt.xlabel(\"Number of Characters\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlim(0, 2000)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table04.to_arrow()[\"n_char_doc\"].to_pandas().hist(edgecolor=\"green\", density=True, facecolor=\"none\", bins=200)\n",
    "table05.to_arrow()[\"n_char_doc\"].to_pandas().hist(edgecolor=\"cyan\", density=True, facecolor=\"none\", bins=200)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_distribution(sorted_counts: list[int]) -> tuple[list[int], list[float]]:\n",
    "    \"\"\"\n",
    "    This function takes a list of values, sorts them in descending order,\n",
    "    and returns the cumulative distribution as a percentage.\n",
    "    \"\"\"\n",
    "    # Sort the character counts in descending order\n",
    "    sorted_counts = sorted(sorted_counts, reverse=True)\n",
    "\n",
    "    # Calculate the cumulative distribution\n",
    "    cumulative_distribution = np.cumsum(sorted_counts)\n",
    "\n",
    "    # Normalize cumulative distribution to get a percentage (0-100%)\n",
    "    cumulative_distribution = cumulative_distribution / cumulative_distribution[-1] * 100\n",
    "\n",
    "    return sorted_counts, cumulative_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "plt.plot(*cumulative_distribution(char_counts01), linewidth=2, color=\"blue\")\n",
    "plt.plot(*cumulative_distribution(char_counts03), linewidth=2, color=\"orange\")\n",
    "plt.plot(*cumulative_distribution(char_counts04), linewidth=2, color=\"green\")\n",
    "plt.plot(*cumulative_distribution(char_counts05), linewidth=2, color=\"cyan\")\n",
    "plt.title(\"Cumulative Distribution\")\n",
    "plt.xlabel(\"Number of Characters (sorted descending)\")\n",
    "plt.ylabel(\"Cumulative Percentage\")\n",
    "plt.xlim(0, 1020)\n",
    "# plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_doc_title vs sim_doc_tags\n",
    "\n",
    "sim_doc_title = table05.to_arrow()[\"sim_doc_title\"]\n",
    "sim_doc_tags = table05.to_arrow()[\"sim_doc_tags\"]\n",
    "\n",
    "# create scatter plot\n",
    "plt.scatter(sim_doc_title, sim_doc_tags, s=2, alpha=0.5)\n",
    "# add 1:1 line\n",
    "plt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, color=\"red\")\n",
    "plt.xlim(-0.2, 1)\n",
    "plt.ylim(-0.2, 1)\n",
    "plt.xlabel(\"sim_doc_title\")\n",
    "plt.ylabel(\"sim_doc_tags\")\n",
    "plt.title(\"sim_doc_title vs sim_doc_tags\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_doc_title = table05.to_arrow()[\"sim_doc_title\"]\n",
    "rank_rel = table05.to_arrow()[\"rank_rel\"]\n",
    "\n",
    "# create scatter plot\n",
    "plt.scatter(sim_doc_title, rank_rel, s=2, alpha=0.5)\n",
    "# add 1:1 line\n",
    "plt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, color=\"red\")\n",
    "plt.xlim(-0.2, 1)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.xlabel(\"sim_doc_title\")\n",
    "plt.ylabel(\"sim_doc_tags\")\n",
    "plt.title(\"sim_doc_title vs rank_rel\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_doc_tags vs n_tags_doc\n",
    "# scatter plot\n",
    "\n",
    "sim_doc_tags = table05.to_arrow()[\"sim_doc_tags\"]\n",
    "n_tags_doc = table05.to_arrow()[\"n_tags_doc\"]\n",
    "\n",
    "# create scatter plot\n",
    "plt.scatter(sim_doc_tags, n_tags_doc, s=2, alpha=0.5)\n",
    "plt.xlim(-0.25, 1)\n",
    "plt.xlabel(\"sim_doc_tags\")\n",
    "plt.ylabel(\"n_tags_doc\")\n",
    "plt.title(\"sim_doc_tags vs n_tags_doc\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
