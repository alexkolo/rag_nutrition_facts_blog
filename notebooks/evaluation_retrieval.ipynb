{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from collections.abc import Callable\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import lancedb\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from lancedb.rerankers import (\n",
    "    CohereReranker,\n",
    "    ColbertReranker,\n",
    "    CrossEncoderReranker,\n",
    "    LinearCombinationReranker,\n",
    "    RRFReranker,\n",
    ")\n",
    "from matplotlib import pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from src.constants import (\n",
    "    GROUND_TRUTH_PATH,\n",
    "    LANCEDB_URI,\n",
    "    POST_JSON_PATH,\n",
    "    REPO_PATH,\n",
    "    get_rag_config,\n",
    ")\n",
    "from src.ingestion import create_title_hash\n",
    "\n",
    "# to ignore FutureWarning from `transformers/tokenization_utils_base.py`\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name: str = get_rag_config()[\"knowledge_base\"][\"table_name\"]\n",
    "ground_truth_file: Path = GROUND_TRUTH_PATH / \"eva_ground_truth.csv\"\n",
    "title_groups_file: Path = GROUND_TRUTH_PATH / \"eva_title_groups.csv\"\n",
    "\n",
    "# secrets\n",
    "load_dotenv(REPO_PATH)\n",
    "cohere_api_key: str = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "\n",
    "# ignore titles for the evaluation that contain these strings\n",
    "ignore_titles: list[str] = [\n",
    "    \"Year-in-Review Presentation\",\n",
    "    \"Thank You for Your Support\",\n",
    "    \"DVD Now Available\",\n",
    "    \"Dr. Greger’s New DVD\",\n",
    "    \"Dr. Greger’s DVDs\",\n",
    "    \"Top 10 NutritionFacts.org Videos\",\n",
    "    \"Top 10 Most Popular Videos\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db: lancedb.db.DBConnection = lancedb.connect(uri=LANCEDB_URI)\n",
    "print(f\"List of all tables in the LanceDB database: {db.table_names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table: lancedb.table.Table = db.open_table(table_name)\n",
    "print(f\"Number of entries in the table '{table_name}': {table.count_rows()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full-text search (aka keyword-based search)\n",
    "# - https://lancedb.github.io/lancedb/fts/\n",
    "# table.create_fts_index([\"text\"], replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector search Index\n",
    "# device: str = get_rag_config()[\"embeddings\"][\"device\"]\n",
    "# emb_model_metric: str = get_rag_config()[\"embeddings\"][\"metric\"]\n",
    "# table.create_index(metric=emb_model_metric, replace=True, accelerator=\"cuda\" if device == \"cuda\" else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Blog entry titles as evaluation queries\n",
    "\n",
    "- try to find titles that are very similar to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity Ranges\n",
    "\n",
    "- **Cosine Similarity = 1.0**: The titles are **exactly identical**.\n",
    "- **Cosine Similarity > 0.9**: The titles are **highly similar**, often differing only slightly, such as small word variations, synonyms, or rewording that conveys essentially the same idea. This is where titles likely \"feel identical\" to most readers.\n",
    "- **Cosine Similarity between 0.7 and 0.9**: The titles are **similar but not identical**. They likely discuss similar topics or concepts but may have different wording or slight shifts in focus. Readers may perceive these as very similar or interchangeable, but subtle differences might exist.\n",
    "- **Cosine Similarity < 0.7**: The titles are **noticeably different**. They may refer to related topics but convey distinct ideas or focus. Readers would likely view them as different, though related.\n",
    "\n",
    "Threshold for Perceived Identicalness: For most **human readers**, a cosine similarity score between **0.9 and 1.0** typically corresponds to titles that feel **identical or extremely similar**. This range indicates that the titles share most of their key terms and structure, with only minor variations.\n",
    "\n",
    "\n",
    "#### Factors Affecting Perceived Similarity:\n",
    "- **Synonyms and Phrasing**: Different words or phrases with similar meanings (e.g., \"guide\" vs. \"how to\").\n",
    "- **Order of Words**: Titles with the same words but in a different order can have a high similarity score and feel identical to readers.\n",
    "- **Stopwords**: Removing common words (e.g., \"the,\" \"a\") in pre-processing can affect the similarity score but might not affect the reader's perception.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cosine Similarity between Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all titles (1281 entries)\n",
    "file_list = list(POST_JSON_PATH.glob(\"*.json\"))\n",
    "\n",
    "\n",
    "ignore_list_lower: list[str] = [title.lower() for title in ignore_titles]\n",
    "\n",
    "titles: list[str] = []\n",
    "urls: list[str] = []\n",
    "created: list[str] = []\n",
    "pbar = tqdm(file_list[:])\n",
    "for json_file in pbar:\n",
    "    pbar.set_description(f\"{json_file.name[:40]:<40}\")\n",
    "\n",
    "    # load data\n",
    "    with open(json_file, encoding=\"utf-8\") as f:\n",
    "        blog_post: dict = json.load(f)\n",
    "    title: str = blog_post[\"title\"]\n",
    "\n",
    "    # ignore files without any paragraphs\n",
    "    if not blog_post[\"paragraphs\"]:\n",
    "        continue\n",
    "\n",
    "    # ignore tiles that contain string from `ignore_titles`\n",
    "    if any(ignored_str in title.lower() for ignored_str in ignore_list_lower):\n",
    "        continue\n",
    "\n",
    "    titles.append(blog_post[\"title\"])\n",
    "    urls.append(blog_post[\"url\"])\n",
    "    created.append(blog_post[\"created\"])\n",
    "# <4 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of titles: {len(titles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure similarity between titles\n",
    "emb_config: dict = get_rag_config()[\"embeddings\"]\n",
    "emb_model = SentenceTransformer(\n",
    "    emb_config[\"model_name\"],\n",
    "    device=emb_config[\"device\"],\n",
    "    similarity_fn_name=emb_config[\"metric\"],\n",
    ")\n",
    "titles_vec: list[list[float]] = emb_model.encode(titles, show_progress_bar=True)\n",
    "# <11 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_sim = pd.DataFrame(emb_model.similarity(titles_vec, titles_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plotting similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the similarity scores as pairs\n",
    "title_pairs = []\n",
    "similarity_scores = []\n",
    "for i in trange(titles_sim.shape[0]):\n",
    "    for j in range(i + 1, titles_sim.shape[1]):\n",
    "        title_pairs.append((titles_sim.index[i], titles_sim.columns[j]))\n",
    "        similarity_scores.append(titles_sim.iloc[i, j])\n",
    "# <20 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.scatter(range(len(similarity_scores)), similarity_scores, alpha=0.5, s=1)\n",
    "plt.title(\"Pairwise Similarity Scores Between Titles\")\n",
    "plt.xlabel(\"Title Pairs (Index)\")\n",
    "plt.ylabel(\"Similarity Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute average histogram and bin edges\n",
    "\n",
    "\n",
    "def compute_avg_hist(df: pd.DataFrame, bins=30, range_min=0, range_max=1, cumulative=False):\n",
    "    # Step 1: Create consistent bin edges for all titles\n",
    "    bin_edges = np.linspace(range_min, range_max, bins + 1)\n",
    "\n",
    "    # Step 2: Initialize list to hold the histograms\n",
    "    hist_data = []\n",
    "\n",
    "    # Set the diagonal values to NaN or exclude them\n",
    "    df_no_diag = df.copy()\n",
    "    np.fill_diagonal(df_no_diag.values, np.nan)\n",
    "\n",
    "    for i in range(df_no_diag.shape[0]):\n",
    "        # Exclude the NaN values (self-similarity) from the row\n",
    "        row_data = df_no_diag.iloc[i, :].dropna()\n",
    "\n",
    "        # Step 3: Compute the regular histogram for this row\n",
    "        hist, _ = np.histogram(row_data, bins=bin_edges, density=True)\n",
    "\n",
    "        if cumulative:\n",
    "            # Step 4: Compute the cumulative sum, starting from the highest bin to the lowest\n",
    "            hist = np.cumsum(hist[::-1])[::-1]  # Reverse to high-to-low cumulative sum\n",
    "\n",
    "        hist_data.append(hist)\n",
    "\n",
    "    # Step 5: Convert the list of histograms into a NumPy array for easier averaging\n",
    "    hist_data = np.array(hist_data)\n",
    "\n",
    "    # Step 6: Compute the average histogram across all rows\n",
    "    avg_hist = hist_data.mean(axis=0)\n",
    "\n",
    "    return avg_hist, bin_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average histogram and bin edges\n",
    "avg_hist, bin_edges = compute_avg_hist(titles_sim, bins=100, range_min=titles_sim.min().min(), range_max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the averaged histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "x_bar = (bin_edges[:-1] + bin_edges[1:]) / 2  # Calculate the bin centers\n",
    "plt.bar(x=x_bar, height=avg_hist, width=(bin_edges[1] - bin_edges[0]), edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Averaged Histogram of Similarity Scores for All Titles\")\n",
    "plt.xlabel(\"Similarity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average Cumulative  histogram and bin edges\n",
    "avg_hist, bin_edges = compute_avg_hist(\n",
    "    titles_sim, bins=100, range_min=titles_sim.min().min(), range_max=1, cumulative=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the averaged histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "x_bar = (bin_edges[:-1] + bin_edges[1:]) / 2  # Calculate the bin centers\n",
    "height = avg_hist  # / avg_hist[0]\n",
    "plt.bar(x=x_bar, height=height, width=(bin_edges[1] - bin_edges[0]), edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Averaged Cumulative Histogram of Similarity Scores (High to Low)\")\n",
    "plt.xlabel(\"Similarity\")\n",
    "plt.ylabel(\"Cumulative Frequency (Density)\")\n",
    "plt.yscale(\"log\")\n",
    "# horizaontl line at one\n",
    "plt.axhline(y=1, color=\"r\", linestyle=\"-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find groups of titles with high Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_titles_by_similarity(df: pd.DataFrame, threshold: float = 0.9) -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    Groups titles based on cosine similarity scores in the DataFrame `df`.\n",
    "    Titles are grouped together if their similarity score is greater than `threshold`.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing pairwise similarity scores, where rows and columns represent titles.\n",
    "    threshold (float): The similarity score threshold for grouping titles (default is 0.9).\n",
    "\n",
    "    Returns:\n",
    "    List of lists, where each sublist contains indices of grouped titles.\n",
    "    \"\"\"\n",
    "    # Create a graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes (each title is a node)\n",
    "    titles = df.index\n",
    "    G.add_nodes_from(titles)\n",
    "\n",
    "    # Add edges between nodes if the similarity score is above the threshold\n",
    "    for i in trange(df.shape[0]):\n",
    "        for j in range(i + 1, df.shape[1]):\n",
    "            if df.iloc[i, j] > threshold:\n",
    "                G.add_edge(titles[i], titles[j])\n",
    "\n",
    "    # Find connected components (groups of titles) and filter out groups with only one title\n",
    "    groups = [group for component in nx.connected_components(G) if len(group := list(component)) > 1]\n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups: list[list[int]] = group_titles_by_similarity(titles_sim, threshold=0.8)\n",
    "# <30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the grouped titles\n",
    "print(f\"Number of groups: {len(groups)}\")\n",
    "for g, group in enumerate(groups):\n",
    "    print(f\"{g}. Group:\", group)\n",
    "    dates = [datetime.fromisoformat(created[i]) for i in group]\n",
    "    # order by date\n",
    "    order = np.argsort(dates)\n",
    "    group = [group[i] for i in order]\n",
    "\n",
    "    i_ref: int = group[-1]  # take most recent title\n",
    "    for i_title in group:\n",
    "        print(f\"\\t({titles_sim.iloc[i_ref, i_title]:.2f}) ({created[i_title][:10]})  {titles[i_title]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save groups\n",
    "out = {}\n",
    "for g, group in enumerate(groups):\n",
    "    # print(f\"{g}. Group:\", group)\n",
    "    dates = [datetime.fromisoformat(created[i]) for i in group]\n",
    "    # order by date\n",
    "    order = np.argsort(dates)\n",
    "    group = [group[i] for i in order]\n",
    "    out[g] = [[create_title_hash({\"title\": titles[i_title], \"url\": urls[i_title]}) for i_title in group]]\n",
    "\n",
    "pd.DataFrame().from_dict(out, orient=\"index\").to_csv(title_groups_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test hash filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with expected\n",
    "groups_from_file = pd.read_csv(title_groups_file)\n",
    "for g, group in enumerate(groups):\n",
    "    expected = {titles[i_title] for i_title in group}\n",
    "    hash_filter: str = groups_from_file.iloc[g, 0].replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    resp = set(table.search().where(f\"hash_title in ({hash_filter})\").limit(100).to_pandas()[\"title\"].to_list())\n",
    "    if resp != expected:\n",
    "        print(f\"Expected: {expected}\")\n",
    "        print(f\"Response: {resp}\")\n",
    "        print(hash_filter)\n",
    "        group_urls = [urls[i_title] for i_title in group]\n",
    "        print(group_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create ground truth\n",
    "\n",
    "- csv: title, hash_title, allowed_hashes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_list(s: str) -> list[str]:\n",
    "    return s.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").split(\", \")\n",
    "\n",
    "\n",
    "title_groups: list[list[str]] = pd.read_csv(title_groups_file).iloc[:, 0].apply(str_to_list).to_list()\n",
    "\n",
    "ground_truth: list[dict] = []\n",
    "for title, url in tqdm(zip(titles, urls), total=len(titles)):\n",
    "    hash_title: str = create_title_hash({\"title\": title, \"url\": url})\n",
    "\n",
    "    # get has of text chunk that has the highest similarity to `title`\n",
    "    best_doc_dict: dict[str, float | str] = (\n",
    "        table.search()\n",
    "        .where(f\"hash_title == '{hash_title}'\")\n",
    "        .limit(100)\n",
    "        .to_pandas()[[\"sim_doc_title\", \"hash_doc\"]]\n",
    "        .sort_values(by=\"sim_doc_title\", ascending=False)\n",
    "        .iloc[0]\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    # check if hash appears in any group from `title_groups` and if yes, add group hashes to `allowed_hases`\n",
    "    allowed_hashes: set[str] = {hash_title}\n",
    "    for group in title_groups:\n",
    "        if hash_title in group:\n",
    "            allowed_hashes.update(group)\n",
    "\n",
    "    ground_truth.append(\n",
    "        {\n",
    "            \"title\": title.strip(),\n",
    "            \"hash_title\": hash_title,\n",
    "            \"allowed_hashes\": list(allowed_hashes),\n",
    "            \"best_doc_hash\": best_doc_dict[\"hash_doc\"],\n",
    "            \"best_doc_sim\": best_doc_dict[\"sim_doc_title\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# save to file\n",
    "# pd.DataFrame(ground_truth).to_csv(ground_truth_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_limit = 0.6\n",
    "best_doc_sim = pd.read_csv(ground_truth_file)[\"best_doc_sim\"]\n",
    "# compute fraction where best_doc_sim < 0.5\n",
    "n_below: int = sum(best_doc_sim < sim_limit)\n",
    "frac = n_below / len(best_doc_sim)\n",
    "print(f\"Fraction of titles with best_doc_sim < {sim_limit}: {frac:.0%} ({n_below} out of {len(best_doc_sim)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \"best_doc_sim\" as cumulative histogram\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.hist(best_doc_sim, bins=100, cumulative=True, density=True, histtype=\"step\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever Evaluation\n",
    "\n",
    "- titles are user queries\n",
    "- a query should return a text chunk from the same blog post or from another blog post with similar cosine similarity in respect to the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_relevance(\n",
    "    ground_truth: list[dict],\n",
    "    search_func: Callable[[], pd.DataFrame],\n",
    "    **kwargs,\n",
    ") -> list[list[bool]]:\n",
    "    relevance_total: list[list[bool]] = []\n",
    "    for entry in tqdm(ground_truth):\n",
    "        allowed_hashes: list[str] = entry[\"allowed_hashes\"]\n",
    "\n",
    "        title: str = entry[\"title\"]\n",
    "        query: str = title.lower().replace(\":\", \" \").replace(\"\\u00a0\", \" \").strip()\n",
    "        resp: pd.DataFrame = search_func(query=query, **kwargs)\n",
    "        retrieved_hashes: list[str] = resp[\"hash_title\"].to_list()\n",
    "        relevance: list[bool] = [h in allowed_hashes for h in retrieved_hashes]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return relevance_total\n",
    "\n",
    "\n",
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] is True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.read_csv(ground_truth_file).to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic search methods: keyword (fts), vector, hybrid\n",
    "\n",
    "- hybrid uses \"Reciprocal Rank Fusion Reranker\"\n",
    "\n",
    "```python\n",
    "100%|██████████| 1248/1248 [00:10<00:00, 119.64it/s]\n",
    "fts - 0:00:10.434329 - 0.819 - 0.948\n",
    "100%|██████████| 1248/1248 [00:31<00:00, 39.56it/s]\n",
    "vector - 0:00:31.548761 - 0.809 - 1.055\n",
    "100%|██████████| 1248/1248 [00:42<00:00, 29.12it/s]\n",
    "hybrid - 0:00:42.865921 - 0.855 - 1.036\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_search(query: str, query_type: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    return (\n",
    "        table.search(query=query, query_type=query_type)\n",
    "        .limit(15)\n",
    "        .to_pandas()\n",
    "        .drop_duplicates(subset=\"hash_doc\")\n",
    "        .head(5)\n",
    "    )\n",
    "\n",
    "\n",
    "res_basic_search: dict[str, list[list[bool]]] = {}\n",
    "for query_type in [\"fts\", \"vector\", \"hybrid\"]:\n",
    "    t_start = datetime.now()\n",
    "    relevance: list[list[bool]] = measure_relevance(ground_truth, search_func=basic_search, query_type=query_type)\n",
    "    t_dur = datetime.now() - t_start\n",
    "    res_basic_search[query_type] = relevance\n",
    "    print(f\"{query_type} - {t_dur} - {hit_rate(relevance):.3f} - {mrr(relevance):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reranker for hybrid search: `LinearCombinationReranker`\n",
    "- https://lancedb.github.io/lancedb/reranking/linear_combination/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search_lc(query: str, top_k: int = 5, weight=0.7) -> pd.DataFrame:\n",
    "    reranker_lc = LinearCombinationReranker(weight=weight)\n",
    "    return (\n",
    "        table.search(query=query, query_type=\"hybrid\")\n",
    "        .rerank(reranker=reranker_lc)\n",
    "        .limit(15)\n",
    "        .to_pandas()\n",
    "        .drop_duplicates(subset=\"hash_doc\")\n",
    "        .head(5)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_lc05: list[list[bool]] = measure_relevance(ground_truth, search_func=hybrid_search_lc, weight=0.5)\n",
    "hit_rate(relevance_lc05), mrr(relevance_lc05)\n",
    "# 100%|██████████| 1248/1248 [00:54<00:00, 22.77it/s]\n",
    "# (0.8549679487179487, 1.035603632478638)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reranker: Reciprocal Rank Fusion \n",
    "https://lancedb.github.io/lancedb/reranking/rrf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search_rrf(query: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    reranker = RRFReranker()\n",
    "    return (\n",
    "        table.search(query=query, query_type=\"hybrid\")\n",
    "        .rerank(reranker=reranker)\n",
    "        .limit(15)\n",
    "        .to_pandas()\n",
    "        .drop_duplicates(subset=\"hash_doc\")\n",
    "        .head(5)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_rrf: list[list[bool]] = measure_relevance(ground_truth, search_func=hybrid_search_rrf)\n",
    "hit_rate(relevance_rrf), mrr(relevance_rrf)\n",
    "# 100%|██████████| 1248/1248 [00:38<00:00, 32.26it/s]\n",
    "# (0.8565705128205128, 1.0644364316239363)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reranker: Cross Encoder\n",
    "\n",
    "- https://lancedb.github.io/lancedb/reranking/cross_encoder/\n",
    "\n",
    "- Models: https://www.sbert.net/docs/pretrained-models/ce-msmarco.html\n",
    "    - Default of LanceDB: `cross-encoder/ms-marco-TinyBERT-L-6` 680 docs / sec (and only version 1)\n",
    "        - too slow! >1h for \"fts\"\n",
    "\n",
    "    - `cross-encoder/ms-marco-TinyBERT-L-2-v2` fastest, 9000 docs / sec (version 2)\n",
    "        - same speed as without reranker but scores are a bit better\n",
    "\n",
    "\n",
    "    - `cross-encoder/ms-marco-MiniLM-L-2-v2` 4100 docs / sec (version 2)\n",
    "        - factor 2 slower but best performance, beating rrf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name: str = \"cross-encoder/ms-marco-MiniLM-L-2-v2\"\n",
    "reranker = CrossEncoderReranker(model_name=model_name, device=\"cpu\")\n",
    "\n",
    "\n",
    "def search_rr_cross(query: str, query_type: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    return table.search(query=query, query_type=query_type).limit(10).rerank(reranker=reranker).limit(top_k).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_cross_res: dict[str, list[list[bool]]] = {}\n",
    "for q_type in [\"fts\", \"vector\", \"hybrid\"]:\n",
    "    rr_cross_res[q_type] = measure_relevance(ground_truth, search_func=search_rr_cross, query_type=q_type)\n",
    "    print(q_type, hit_rate(rr_cross_res[q_type]), mrr(rr_cross_res[q_type]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates: slower, lower hit rate, higher MMR\n",
    "model_name: str = \"cross-encoder/ms-marco-MiniLM-L-2-v2\"\n",
    "cross_encoder = CrossEncoderReranker(model_name=model_name, device=\"cpu\")\n",
    "\n",
    "\n",
    "def search_rr_cross02(query: str, query_type: str) -> pd.DataFrame:\n",
    "    return (\n",
    "        table.search(query=query, query_type=query_type)\n",
    "        # .rerank(reranker=rrf)\n",
    "        # .limit(10)\n",
    "        .rerank(reranker=cross_encoder)\n",
    "        .limit(15)\n",
    "        .to_pandas()\n",
    "        .drop_duplicates(subset=\"hash_doc\")\n",
    "        .head(5)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_cross02 = measure_relevance(ground_truth, search_func=search_rr_cross02, query_type=\"hybrid\")\n",
    "print(hit_rate(rr_cross02), mrr(rr_cross02))\n",
    "# 100%|██████████| 1248/1248 [08:40<00:00,  2.40it/s]\n",
    "# 0.8790064102564102 1.0757879273504316"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reranker Chain: RRF -> Cross Encoder \n",
    "\n",
    "- no difference than using Cross Encoder directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rrf = RRFReranker()\n",
    "model_name: str = \"cross-encoder/ms-marco-MiniLM-L-2-v2\"\n",
    "cross_encoder = CrossEncoderReranker(model_name=model_name, device=\"cpu\")\n",
    "\n",
    "\n",
    "def search_rr_chain(query: str, query_type: str) -> pd.DataFrame:\n",
    "    return (\n",
    "        table.search(query=query, query_type=query_type)\n",
    "        # .rerank(reranker=rrf)\n",
    "        # .limit(10)\n",
    "        .rerank(reranker=cross_encoder)\n",
    "        .limit(15)\n",
    "        .to_pandas()\n",
    "        .drop_duplicates(subset=\"hash_doc\")\n",
    "        .head(5)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_chain = measure_relevance(ground_truth, search_func=search_rr_chain, query_type=\"hybrid\")\n",
    "print(hit_rate(rr_chain), mrr(rr_chain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "```test\n",
    "               hit_rate       mrr\n",
    "cross_encoder  0.879006  1.075788 (hybrid + reranker)\n",
    "rrf            0.856571  1.064436 (hybrid + reranker)\n",
    "hybrid         0.854968  1.035604\n",
    "lc_w0.5        0.854968  1.035604 (hybrid + reranker)\n",
    "fts            0.818910  0.947983\n",
    "vector         0.809295  1.054688\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_all = {}\n",
    "res_all.update(res_basic_search)\n",
    "res_all[\"rrf\"] = relevance_rrf\n",
    "res_all[\"lc_w0.5\"] = relevance_lc05\n",
    "res_all[\"cross_encoder\"] = rr_cross02  # rr_cross_res[\"hybrid\"]\n",
    "scores_all = {key: {\"hit_rate\": hit_rate(res_all[key]), \"mrr\": mrr(res_all[key])} for key in res_all}\n",
    "pd.DataFrame.from_dict(scores_all, orient=\"index\").sort_values(by=\"hit_rate\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reranker: Coher (rate limit too low)\n",
    "- https://lancedb.github.io/lancedb/reranking/cohere/\n",
    "- only trial api key:  10 API calls / minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_rr_cohere(query: str, query_type: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    reranker = CohereReranker(api_key=cohere_api_key)\n",
    "    return table.search(query=query, query_type=query_type).rerank(reranker=reranker).limit(top_k).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_cohere_res: dict[str, list[list[bool]]] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for q_type in [\"fts\"]:  # , \"vector\", \"hybrid\"]:\n",
    "#     rr_cohere_res[q_type] = measure_relevance(ground_truth, search_func=search_rr_cohere, query_type=q_type)\n",
    "#     print(q_type, hit_rate(rr_cohere_res[q_type]), mrr(rr_cohere_res[q_type]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reranker: ColBERT  (slow)\n",
    "- https://lancedb.github.io/lancedb/reranking/colbert/\n",
    "- Evaluation  slow! >30min for fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_rr_colbert(query: str, query_type: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    reranker = ColbertReranker()  # device=\"cpu\")\n",
    "    return table.search(query=query, query_type=query_type).limit(10).rerank(reranker=reranker).limit(top_k).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_colbert_res: dict[str, list[list[bool]]] = {}\n",
    "for q_type in [\"hybrid\"]:  # \"fts\", \"vector\", ]:\n",
    "    rr_colbert_res[q_type] = measure_relevance(ground_truth, search_func=search_rr_colbert, query_type=q_type)\n",
    "    print(q_type, hit_rate(rr_colbert_res[q_type]), mrr(rr_colbert_res[q_type]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
