{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Scraping Blog Posts off `nutritionfacts.org`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import time\n",
                "from pathlib import Path\n",
                "\n",
                "from bs4 import BeautifulSoup\n",
                "from tqdm import tqdm\n",
                "\n",
                "from src.web_scraping import (\n",
                "    extract_all_urls,\n",
                "    extract_blog_data,\n",
                "    filter_links,\n",
                "    get_meta_data,\n",
                "    get_paragraphs,\n",
                "    get_webpage_content,\n",
                "    replace_strange_chars,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_path = Path(\".\").resolve().parent / \"data\"\n",
                "data_path.is_dir()  # fails if it doesn't exist\n",
                "blog_posts_root: Path = data_path / \"blog_posts\"\n",
                "post_path_raw: Path = blog_posts_root / \"raw_txt\"\n",
                "post_path_raw.is_dir()  # fails if it doesn't exist\n",
                "post_path_json: Path = blog_posts_root / \"json\"\n",
                "post_path_json.is_dir()  # fails if it doesn't exist"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "root_url: str = \"https://nutritionfacts.org/blog/\"\n",
                "file_url_list: Path = blog_posts_root / \"blog_posts_urls.csv\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Code"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Testing connection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "response = get_webpage_content(root_url)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Parse the HTML content\n",
                "soup = BeautifulSoup(response.content, \"html.parser\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find all links on the page\n",
                "links: set[str] = sorted({link[\"href\"] for link in soup.find_all(\"a\", href=True)})\n",
                "print(\"Number of links:\", len(links))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# filter the links\n",
                "blog_posts_of_page: list[str] = filter_links(links, root_url)\n",
                "n_posts: int = len(blog_posts_of_page)\n",
                "print(f\"Number of blog posts: {n_posts}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extract urls of all blog posts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "urls_list: list[str] = extract_all_urls(root=root_url, page_stop=None)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "blog_post_urls_set = set(urls_list)\n",
                "print(\"Number of unique blog posts:\", len(blog_post_urls_set))\n",
                "# Number of blog posts: 1285"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# post processing\n",
                "for url in list(blog_post_urls_set):  # create a copy of the set\n",
                "    link_tail: str = url.replace(root_url, \"\").replace(\"/\", \"\")\n",
                "    # remove some urls that are not blog posts\n",
                "    if link_tail.isdigit():\n",
                "        print(url)\n",
                "        blog_post_urls_set.remove(url)\n",
                "print(\"Number of unique blog posts:\", len(blog_post_urls_set))\n",
                "# Number of unique blog posts: 1281"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# export to csv file\n",
                "with open(blog_posts_root / file_url_list, \"w\") as f:\n",
                "    for url in sorted(blog_post_urls_set):\n",
                "        f.write(f\"{url}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extract content of each blog post"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# read from csv file\n",
                "with open(blog_posts_root / file_url_list) as f:\n",
                "    urls_list: list[str] = f.read().splitlines()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "blog_post_url = urls_list[1111]\n",
                "url_tail = blog_post_url.replace(root_url, \"\").replace(\"/\", \"\")\n",
                "url_tail"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "blog_post_url"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "response = get_webpage_content(blog_post_url)\n",
                "# Parse the HTML content\n",
                "soup = BeautifulSoup(response.content, \"html.parser\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# write to file\n",
                "with open(f\"{url_tail}.html\", \"w\") as f:\n",
                "    f.write(str(soup))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### pure content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract the content you are interested in\n",
                "paragraphs_raw = soup.find_all(\"p\", class_=\"p1\")\n",
                "content = \"\\n\\n\".join(para.get_text() for para in paragraphs_raw)\n",
                "paragraphs_raw"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(f\"{url_tail}.txt\", \"w\") as f:\n",
                "    f.write(content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### meta data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "meta_data = get_meta_data(soup)\n",
                "meta_data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "title_text = soup.find(\"h1\", class_=\"entry-title\").get_text()\n",
                "title_text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract the first datetime value\n",
                "date_created = soup.find(\"time\", class_=\"updated\")[\"datetime\"]\n",
                "\n",
                "# Extract the second datetime value (using the second <time> tag)\n",
                "date_last_update = soup.find_all(\"time\")[1][\"datetime\"]\n",
                "\n",
                "print(\"Datetime 01:\", date_created)\n",
                "print(\"Datetime 02:\", date_last_update)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### paragraphs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "paragraphs_clean = get_paragraphs(soup)\n",
                "paragraphs_clean"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "paragraphs_html: list = soup.find_all(\"p\", class_=\"p1\")\n",
                "if not paragraphs_html:\n",
                "    paragraphs_html = soup.find_all(\"p\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "paragraphs_raw: list[str] = [para.get_text() for para in paragraphs_html]\n",
                "paragraphs_raw"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract and clean paragraphs while excluding those that start with certain phrases\n",
                "paragraphs_raw: list[str] = [para_html.get_text().strip() for para_html in paragraphs_html]\n",
                "exclude_startswith: list[str] = [\n",
                "    \"Written By\",\n",
                "    \"Image Credit\",\n",
                "    \"In health\",\n",
                "    \"Michael Greger\",\n",
                "    \"PS:\",\n",
                "    \"A founding member\",\n",
                "    \"Subscribe\",\n",
                "    \"Catch up\",\n",
                "    \"Charity ID\",\n",
                "    \"We  our volunteers!\",\n",
                "    \"Interested in learning more about\",\n",
                "    \"Check out:\",\n",
                "]\n",
                "# Create clean list\n",
                "paragraphs_clean: list[str] = [\n",
                "    replace_strange_chars(para_raw)\n",
                "    for para_raw in paragraphs_raw\n",
                "    if para_raw and not any(para_raw.startswith(prefix) for prefix in exclude_startswith)\n",
                "]\n",
                "paragraphs_clean"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Extract key takeaways"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "key_takeaways_heading = soup.find(\"p\", string=\"KEY TAKEAWAYS\")\n",
                "if key_takeaways_heading is None:\n",
                "    key_takeaways = []\n",
                "else:\n",
                "    # Find the next <ul> element after the \"KEY TAKEAWAYS\" heading\n",
                "    key_takeaways_list = key_takeaways_heading.find_next(\"ul\")\n",
                "\n",
                "    # Extract the text from each <li> in the list\n",
                "    key_takeaways = [replace_strange_chars(li.get_text().stripe()) for li in key_takeaways_list.find_all(\"li\")]\n",
                "\n",
                "# Print or use the extracted key takeaways\n",
                "for takeaway in key_takeaways:\n",
                "    print(takeaway)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### article tags"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tags_raw = soup.find(\"article\").get(\"class\")\n",
                "if tags_raw:\n",
                "    tags_blog = [tag.split(\"-\")[1] for tag in tags_raw if tag.startswith(\"tag-\")]\n",
                "    print(tags_blog)\n",
                "    cats = [cat.split(\"-\")[1] for cat in tags_raw if cat.startswith(\"category-\")]\n",
                "    print(cats)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### export to json"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "blog_data = extract_blog_data(soup)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# write to json file\n",
                "with open(f\"{url_tail}.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
                "    json.dump(blog_data, json_file, ensure_ascii=True, indent=4)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Real extraction loop"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### pure text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# pure text\n",
                "for url in tqdm(urls_list):\n",
                "    url_tail = url.replace(root_url, \"\").replace(\"/\", \"\")\n",
                "    file_out = post_path_raw / f\"{url_tail}.txt\"\n",
                "    if file_out.exists():\n",
                "        continue\n",
                "\n",
                "    time.sleep(0.5)  # wait a bit to avoid being blocked\n",
                "\n",
                "    # get the HTML content\n",
                "    response = get_webpage_content(url)\n",
                "    # Parse the HTML content\n",
                "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
                "\n",
                "    # Extract the content\n",
                "    paragraphs = soup.find_all(\"p\")\n",
                "    content = \"\\n\\n\".join(para.get_text() for para in paragraphs)\n",
                "\n",
                "    # export to file\n",
                "    with open(file_out, \"w\", encoding=\"utf-8\") as f:\n",
                "        f.write(content)\n",
                "\n",
                "# 100%|██████████| 1281/1281 [28:03<00:00,  1.31s/it]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### meta data and text chunks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for url in tqdm(urls_list):\n",
                "    url_tail = url.replace(root_url, \"\").replace(\"/\", \"\")\n",
                "    file_out = post_path_json / f\"{url_tail}.json\"\n",
                "    if file_out.exists():\n",
                "        continue\n",
                "\n",
                "    time.sleep(0.1)  # wait a bit to avoid being blocked\n",
                "\n",
                "    # get the HTML content\n",
                "    response = get_webpage_content(url)\n",
                "\n",
                "    # Parse the HTML content\n",
                "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
                "\n",
                "    # Extract the blog data\n",
                "    blog_data: dict = {\"url\": url}\n",
                "    blog_data.update(extract_blog_data(soup))\n",
                "\n",
                "    # export to json file\n",
                "    with open(file_out, \"w\", encoding=\"utf-8\") as json_file:\n",
                "        json.dump(blog_data, json_file, ensure_ascii=True, indent=4)\n",
                "# 100%|██████████| 1281/1281 [22:06<00:00,  1.04s/it]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
