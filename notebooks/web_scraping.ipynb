{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Blog Posts off `nutritionfacts.org`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_webpage_content(url) -> requests.Response | None:\n",
    "    try:\n",
    "        response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def filter_links(links: list[str], root: str) -> list[str]:\n",
    "    filtered_links: list[str] = []\n",
    "    for href in links:\n",
    "        if not href.startswith(root):\n",
    "            continue\n",
    "        link_tail: str = href.replace(root, \"\")\n",
    "        if link_tail and not link_tail.startswith(\"page\"):\n",
    "            filtered_links.append(href)\n",
    "\n",
    "    return filtered_links\n",
    "\n",
    "\n",
    "def extract_all_urls(root: str, page_stop: int | None = None) -> list[str]:\n",
    "    # collect all the blog posts urls\n",
    "    i_page: int = 0\n",
    "    url_list: list[str] = []\n",
    "    while True:\n",
    "        time.sleep(0.2)  # wait a bit to avoid being blocked\n",
    "        i_page += 1\n",
    "        # for debug only\n",
    "        if page_stop is not None and i_page > page_stop:\n",
    "            break\n",
    "\n",
    "        if i_page == 1:\n",
    "            page_url = root\n",
    "        else:\n",
    "            page_url = f\"{root}page/{i_page}/\"\n",
    "        print(f\"{i_page}. Page URL: {page_url}\")\n",
    "\n",
    "        # get the HTML content\n",
    "        response = get_webpage_content(page_url)\n",
    "        if response is None:\n",
    "            break\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # get all links on the page\n",
    "        links: list[str] = sorted(\n",
    "            {link[\"href\"] for link in soup.find_all(\"a\", href=True)}\n",
    "        )\n",
    "\n",
    "        # filter the links\n",
    "        blog_posts_of_page: list[str] = filter_links(links, root)\n",
    "        n_posts: int = len(blog_posts_of_page)\n",
    "        print(f\"\\t Number of blog posts: {n_posts}\")\n",
    "        # page needs at least 2 posts to be considered, otherwise it's at the last page\n",
    "        if n_posts < 2:\n",
    "            break\n",
    "        url_list.extend(blog_posts_of_page)\n",
    "\n",
    "    return url_list\n",
    "\n",
    "\n",
    "def replace_strange_chars(text: str) -> str:\n",
    "    # Create a dictionary for replacements to make the code more scalable\n",
    "    replacements = {\n",
    "        \"“\": \"'\",\n",
    "        \"”\": \"'\",\n",
    "        \"’\": \"'\",\n",
    "        \"‘\": \"'\",\n",
    "        \"…\": \"...\",\n",
    "        \"—\": \"-\",\n",
    "        \"\\u00a0\": \" \",\n",
    "    }\n",
    "    return text.translate(str.maketrans(replacements))\n",
    "\n",
    "\n",
    "def get_meta_data(soup: BeautifulSoup) -> dict:\n",
    "    meta_data = {\n",
    "        \"title\": soup.find(\"h1\", class_=\"entry-title\").get_text(),\n",
    "        \"created\": soup.find(\"time\", class_=\"updated\")[\"datetime\"],\n",
    "        \"updated\": soup.find_all(\"time\")[1][\"datetime\"],\n",
    "    }\n",
    "    return meta_data\n",
    "\n",
    "\n",
    "def get_paragraphs(soup: BeautifulSoup) -> list[str]:\n",
    "    paragraphs_html: list = soup.find_all(\"p\", class_=\"p1\")\n",
    "    if not paragraphs_html:\n",
    "        paragraphs_html = soup.find_all(\"p\")\n",
    "\n",
    "    # Extract and clean paragraphs while excluding those that start with certain phrases\n",
    "    paragraphs_raw: list[str] = [\n",
    "        replace_strange_chars(para_html.get_text().strip())\n",
    "        for para_html in paragraphs_html\n",
    "    ]\n",
    "    exclude_startswith: list[str] = [\n",
    "        \"Written By\",\n",
    "        \"Image Credit\",\n",
    "        \"In health\",\n",
    "        \"Michael Greger\",\n",
    "        \"-Michael Greger\",\n",
    "        \"PS:\",\n",
    "        \"A founding member\",\n",
    "        \"Subscribe\",\n",
    "        \"Catch up\",\n",
    "        \"Charity ID\",\n",
    "        \"We  our volunteers!\",\n",
    "        \"Interested in learning more about\",\n",
    "        \"Check out\",\n",
    "        \"For more on\",\n",
    "    ]\n",
    "    # Create clean list\n",
    "    paragraphs_clean: list[str] = [\n",
    "        para_raw\n",
    "        for para_raw in paragraphs_raw\n",
    "        if para_raw\n",
    "        and not any(para_raw.startswith(prefix) for prefix in exclude_startswith)\n",
    "    ]\n",
    "    return paragraphs_clean\n",
    "\n",
    "\n",
    "def get_key_takeaways(soup: BeautifulSoup) -> list[str]:\n",
    "    key_takeaways_heading = soup.find(\"p\", string=\"KEY TAKEAWAYS\")\n",
    "    if key_takeaways_heading is None:\n",
    "        return []\n",
    "\n",
    "    # Find the next <ul> element after the \"KEY TAKEAWAYS\" heading\n",
    "    key_takeaways_list = key_takeaways_heading.find_next(\"ul\")\n",
    "\n",
    "    # Extract the text from each <li> in the list\n",
    "    return [\n",
    "        replace_strange_chars(li.get_text().strip())\n",
    "        for li in key_takeaways_list.find_all(\"li\")\n",
    "    ]\n",
    "\n",
    "\n",
    "def extract_blog_data(soup: BeautifulSoup) -> dict:\n",
    "\n",
    "    blog_content: dict = get_meta_data(soup)\n",
    "\n",
    "    tags_raw = soup.find(\"article\").get(\"class\")\n",
    "    blog_content[\"category\"] = [\n",
    "        cat.split(\"-\")[1] for cat in tags_raw if cat.startswith(\"category-\")\n",
    "    ]\n",
    "    blog_content[\"blog_tags\"] = [\n",
    "        tag.split(\"-\")[1] for tag in tags_raw if tag.startswith(\"tag-\")\n",
    "    ]\n",
    "    blog_content[\"raw_tags\"] = tags_raw\n",
    "\n",
    "    blog_content[\"paragraphs\"] = get_paragraphs(soup)\n",
    "    blog_content[\"key_takeaways\"] = get_key_takeaways(soup)\n",
    "\n",
    "    return blog_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\".\").resolve().parent / \"data\"\n",
    "data_path.is_dir()  # fails if it doesn't exist\n",
    "blog_posts_root: Path = data_path / \"blog_posts\"\n",
    "post_path_raw: Path = blog_posts_root / \"raw_txt\"\n",
    "post_path_raw.is_dir()  # fails if it doesn't exist\n",
    "post_path_json: Path = blog_posts_root / \"json\"\n",
    "post_path_json.is_dir()  # fails if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_url: str = \"https://nutritionfacts.org/blog/\"\n",
    "file_url_list: Path = blog_posts_root / \"blog_posts_urls.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_webpage_content(root_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all links on the page\n",
    "links: set[str] = sorted({link[\"href\"] for link in soup.find_all(\"a\", href=True)})\n",
    "print(\"Number of links:\", len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the links\n",
    "blog_posts_of_page: list[str] = filter_links(links, root_url)\n",
    "n_posts: int = len(blog_posts_of_page)\n",
    "print(f\"Number of blog posts: {n_posts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract urls of all blog posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_list: list[str] = extract_all_urls(root=root_url, page_stop=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_post_urls_set = set(urls_list)\n",
    "print(\"Number of unique blog posts:\", len(blog_post_urls_set))\n",
    "# Number of blog posts: 1285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post processing\n",
    "for url in list(blog_post_urls_set):  # create a copy of the set\n",
    "    link_tail: str = url.replace(root_url, \"\").replace(\"/\", \"\")\n",
    "    # remove some urls that are not blog posts\n",
    "    if link_tail.isdigit():\n",
    "        print(url)\n",
    "        blog_post_urls_set.remove(url)\n",
    "print(\"Number of unique blog posts:\", len(blog_post_urls_set))\n",
    "# Number of unique blog posts: 1281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv file\n",
    "with open(blog_posts_root / file_url_list, \"w\") as f:\n",
    "    for url in sorted(blog_post_urls_set):\n",
    "        f.write(f\"{url}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract content of each blog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from csv file\n",
    "with open(blog_posts_root / file_url_list) as f:\n",
    "    urls_list: list[str] = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_post_url = urls_list[1111]\n",
    "url_tail = blog_post_url.replace(root_url, \"\").replace(\"/\", \"\")\n",
    "url_tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_post_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_webpage_content(blog_post_url)\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "with open(f\"{url_tail}.html\", \"w\") as f:\n",
    "    f.write(str(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pure content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the content you are interested in\n",
    "paragraphs_raw = soup.find_all(\"p\", class_=\"p1\")\n",
    "content = \"\\n\\n\".join(para.get_text() for para in paragraphs_raw)\n",
    "paragraphs_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{url_tail}.txt\", \"w\") as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = get_meta_data(soup)\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_text = soup.find(\"h1\", class_=\"entry-title\").get_text()\n",
    "title_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first datetime value\n",
    "date_created = soup.find(\"time\", class_=\"updated\")[\"datetime\"]\n",
    "\n",
    "# Extract the second datetime value (using the second <time> tag)\n",
    "date_last_update = soup.find_all(\"time\")[1][\"datetime\"]\n",
    "\n",
    "print(\"Datetime 01:\", date_created)\n",
    "print(\"Datetime 02:\", date_last_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_clean = get_paragraphs(soup)\n",
    "paragraphs_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_html: list = soup.find_all(\"p\", class_=\"p1\")\n",
    "if not paragraphs_html:\n",
    "    paragraphs_html = soup.find_all(\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_raw: list[str] = [para.get_text() for para in paragraphs_html]\n",
    "paragraphs_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and clean paragraphs while excluding those that start with certain phrases\n",
    "paragraphs_raw: list[str] = [\n",
    "    para_html.get_text().strip() for para_html in paragraphs_html\n",
    "]\n",
    "exclude_startswith: list[str] = [\n",
    "    \"Written By\",\n",
    "    \"Image Credit\",\n",
    "    \"In health\",\n",
    "    \"Michael Greger\",\n",
    "    \"PS:\",\n",
    "    \"A founding member\",\n",
    "    \"Subscribe\",\n",
    "    \"Catch up\",\n",
    "    \"Charity ID\",\n",
    "    \"We  our volunteers!\",\n",
    "    \"Interested in learning more about\",\n",
    "    \"Check out:\",\n",
    "]\n",
    "# Create clean list\n",
    "paragraphs_clean: list[str] = [\n",
    "    replace_strange_chars(para_raw)\n",
    "    for para_raw in paragraphs_raw\n",
    "    if para_raw\n",
    "    and not any(para_raw.startswith(prefix) for prefix in exclude_startswith)\n",
    "]\n",
    "blog_content[\"paragraphs\"] = paragraphs_clean\n",
    "paragraphs_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract key takeaways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_takeaways_heading = soup.find(\"p\", string=\"KEY TAKEAWAYS\")\n",
    "if key_takeaways_heading is None:\n",
    "    key_takeaways = []\n",
    "else:\n",
    "    # Find the next <ul> element after the \"KEY TAKEAWAYS\" heading\n",
    "    key_takeaways_list = key_takeaways_heading.find_next(\"ul\")\n",
    "\n",
    "    # Extract the text from each <li> in the list\n",
    "    key_takeaways = [\n",
    "        replace_strange_chars(li.get_text().stripe())\n",
    "        for li in key_takeaways_list.find_all(\"li\")\n",
    "    ]\n",
    "\n",
    "# Print or use the extracted key takeaways\n",
    "for takeaway in key_takeaways:\n",
    "    print(takeaway)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### article tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_raw = soup.find(\"article\").get(\"class\")\n",
    "if tags_raw:\n",
    "    tags_blog = [tag.split(\"-\")[1] for tag in tags_raw if tag.startswith(\"tag-\")]\n",
    "    print(tags_blog)\n",
    "    cats = [cat.split(\"-\")[1] for cat in tags_raw if cat.startswith(\"category-\")]\n",
    "    print(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### export to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_data = extract_blog_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to json file\n",
    "with open(f\"{url_tail}.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(blog_data, json_file, ensure_ascii=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real extraction loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pure text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pure text\n",
    "for url in tqdm(urls_list):\n",
    "    url_tail = url.replace(root_url, \"\").replace(\"/\", \"\")\n",
    "    file_out = post_path_raw / f\"{url_tail}.txt\"\n",
    "    if file_out.exists():\n",
    "        continue\n",
    "\n",
    "    time.sleep(0.5)  # wait a bit to avoid being blocked\n",
    "\n",
    "    # get the HTML content\n",
    "    response = get_webpage_content(url)\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract the content\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    content = \"\\n\\n\".join(para.get_text() for para in paragraphs)\n",
    "\n",
    "    # export to file\n",
    "    with open(file_out, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "# 100%|██████████| 1281/1281 [28:03<00:00,  1.31s/it]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### meta data and text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in tqdm(urls_list):\n",
    "    url_tail = url.replace(root_url, \"\").replace(\"/\", \"\")\n",
    "    file_out = post_path_json / f\"{url_tail}.json\"\n",
    "    if file_out.exists():\n",
    "        continue\n",
    "\n",
    "    time.sleep(0.1)  # wait a bit to avoid being blocked\n",
    "\n",
    "    # get the HTML content\n",
    "    response = get_webpage_content(url)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract the blog data\n",
    "    blog_data: dict = {\"url\": url}\n",
    "    blog_data.update(extract_blog_data(soup))\n",
    "\n",
    "    # export to json file\n",
    "    with open(file_out, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(blog_data, json_file, ensure_ascii=True, indent=4)\n",
    "# 100%|██████████| 1281/1281 [22:06<00:00,  1.04s/it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
